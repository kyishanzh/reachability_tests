\documentclass{amsart}
\usepackage[margin=1in]{geometry}

\usepackage{amsthm, amsmath, amssymb}
\usepackage[colorlinks]{hyperref}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{tikz}
\usetikzlibrary{calc,matrix,arrows,decorations.markings}
\usepackage{array}
\usepackage{color}
\usepackage{enumerate}
\usepackage{nicefrac}
\usepackage{listings}
\bibliographystyle{plainurl}
\usepackage{cite}
\usepackage{mathtools}
\usepackage{enumitem}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\lstset{
    basicstyle=\small\ttfamily,
    keywordstyle=\color{blue},
    language=python,
    xleftmargin=16pt,
}

\topmargin=-0.5in
\headheight=0in
\pagestyle{plain}


% ------   Theorem Styles -------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{fact}[theorem]{Fact}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{setup}[theorem]{Setup}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\setlength{\parindent}{0pt}



\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\abss}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\snorm}[1]{\lVert#1\rVert}
\newcommand{\ang}[1]{\left\langle #1 \right\rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\sqb}[1]{\left[ #1 \right]}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\setcond}[2]{\left\{ #1 \;\middle\vert\; #2 \right\}}
\newcommand{\cond}[2]{\left( #1 \;\middle\vert\; #2 \right)}
\newcommand{\sqcond}[2]{\left[ #1 \;\middle\vert\; #2 \right]}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}

\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\raggedbottom

\title{Reachability Project Log :)}
\author[]{}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\section{Problem Description}

Our goal is to learn \textit{where the robot should stand} given a desired grasp. 

\subsection{Problem inputs} Formulating the problem, we are given as inputs:
\begin{itemize}
    \item $x_e \in SE(3)$: desired end-effector pose in world coordinates
    \item $b \in SE(2)$: wheeled mobile base pose $(x, y, \gamma)$
    \item $q \in \mathbb{R}^n$: arm joint configuration.
\end{itemize}

\subsection{Problem outputs} We want to find a range of feasible whole-body configurations that satisfy the following requirements:
\begin{enumerate}
    \item \textbf{IK feasibility:} $\text{forward kinematics}(b, q) = x_e$
    \item \textbf{Visibility/sensing constraints:} The end-effector should lie within the camera's field of view and range (and un-occluded by the robot itself), i.e. $\text{visible}(b, q, x_e) = 1$
    \item \textbf{Ball of feasible configurations:} We want a region in joint space around $(b,q)$ that keeps the hand in approximately the right pose and satisfies the above constraints. 
\end{enumerate}

Our target is to model some distribution $p(b, q\mid x_e)$ subject to these constraints $\uparrow$, ideally with some notion of pose quality (visibility, manipulability, clearance, etc.). 

\section{Simplified problem v2: Round thing with a rotary 2-link arm}

\subsection{Setup}

\subsection{Forward kinematics}

\subsubsection{\texttt{fk\_mse\_from\_qfeat} loss function math}

Input to function: \texttt{mu\_q} ($mu_q$ = model's prediction of the robot's state) and $H$. We want to take this prediction, convert it into a valid physical configuration, calculate where the arms' tip should be in the world and compare that to a target position $H$. 

\subsection{Training}

\subsubsection{Nearest neighbors baseline} 

\texttt{
=== Results: NNDeterministicLookup ===\\
hand\_err/mean                  0.014401\\
hand\_err/median                0.013445\\
hand\_err/p95                   0.028278\\
coverage/max\_gap\_mean          6.283185\\
coverage/max\_gap\_p95           6.283185\\
coverage/kl\_to\_uniform         0.019053\\
stochasticity/var\_Q\_fixed\_H    0.000000}

\subsubsection{Conditional VAE}

\textbf{Metrics to track:}
\begin{itemize}
    \item Total loss
    \item Reconstruction loss term
    \item KL loss term
    \item Mean of $|\mu|$: if near 0 always, encoder might be collapsing
    \item Mean of logvar: if very negative $\rightarrow$ tiny standard deviation $\rightarrow$ near-deterministic encoder
    \item Gradient norm: detect instability or dead training
    \item KL per dimension to show if any of the latent dimensions are ``dead'' (near-zero KL contribution)
\end{itemize}

\subsubsection{Conditional INN}




\section{Simplified problem v1: Round thing with a fixed stick}

\subsection{Setup}

Simple robot is a round thing with a fixed ``stick'' attached to it: a disk on the floor with a rigid stick of fixed length $L$ glued to it, and the stick rotates with the disk.
\begin{itemize}
    \item Configuration: $Q = x, y, \theta$ ($\mathbb{R}^2 \times S^1$)
    \item Hand target: $H = h_x, h_y$ (point in $\mathbb{R}^2$)
\end{itemize}

In this setup, $H \in \mathbb{R}^2$ is the desired 2D point on the floor where we want the tip of the stick to be.

\subsection{Forward kinematics}
\[\text{hand}(x, y, \theta) \coloneq f(Q) = \begin{bmatrix}
    x \\ y
\end{bmatrix} + L \begin{bmatrix}
    \cos \theta & \sin \theta
\end{bmatrix}\]

So the IK constraint (``hand hits the target'') is:
\[H = \begin{bmatrix}
    h_x \\ h_y
\end{bmatrix} = \begin{bmatrix}
    x \\ y 
\end{bmatrix} + L \begin{bmatrix}
    \cos \theta \\ \sin \theta
\end{bmatrix}\]

Rearranging shows that as $\theta$ varies, the base center $(x,y)$ traces out a circle of radius $L$ around the target point $H$: the set of feasible base positions for the robot to reach $H$ is a circle centered at $H$. 
\[\text{Ground truth feasible set for a fixed $H$} \coloneq \{Q: f(Q)= H\}= \{(h_x - L\cos \theta, h_y - L \sin\theta, \theta): \theta \in [0, 2\pi)\}\]

% Simple robot is a round thing with a fixed “stick” attached to it
% - configuration : Q = x, y, theta
% - hand target : H = x, y point

\subsection{Approaches} We try the following approaches to model $p(Q \mid H)$:
\begin{enumerate}
    \item \textit{Nearest neighbors} (baseline):
    \begin{itemize}
        \item Given a dataset $\mathcal{D} = \{(H_i, Q_i)\}_{i=1}^N$: for a query $H'$,
        \begin{enumerate}
            \item Find indices of the $k$ nearest $H_i$ to $H'$ (under $||H_i - H'||$)
            \item Return their associated $Q_i$'s (either all or sample one etc.)
        \end{enumerate}
        \item For a new $H'$, we can return the $Q_i$ associated to the nearest $H_i$, or sample from an empirical conditional distribution created based on the nearest neighbor graph:  $\hat{p}_{\text{kNN}}(Q \mid H') = \sum_{j \in \mathcal{N}_k(H')} w_j (H') \delta (Q - Q_j)$ (where weight $w_j$ is proportional to the distance between $H'$ and $H_i$, i.e. $w_j \propto \exp(-||H_j - H'||^2 / \sigma^2)$).
    \end{itemize}
    \item \textit{Conditional VAE:} Introduce latent $z \in \mathbb{R}^d$: $z \sim \mathcal{N}(0, I), \,\, Q \sim p_\theta(Q \mid H, z)$
    \begin{itemize}
        \item Train an encoder $q_\phi(z \mid Q, H)$ and decoder $p_\theta(Q \mid H, z)$ via ELBO loss:
        \[\log p_\theta (Q \mid H) \geq \mathbb{E}_{z \sim q_\phi(z \mid Q, H)} [\log p_\theta (Q \mid H, z)] - \mathrm{KL}[q_\theta(z \mid Q, H)\, || \,\mathcal{N}(0, I)]\]
        \item Potential decoder choice: output $\mu_\theta(H, z)$ and diagonal $\Sigma_\theta(H, z)$; use Gaussian likelihood: \[\log p_\theta (Q \mid H, z) = \log \mathcal{N}(Q; \mu_\theta (H, z), \Sigma_\theta (H, z))\]
    \end{itemize}
    \textcolor{red}{[TODO: Read into math more]}, notes in \ref{cvae}
    \begin{itemize}
        \item Advantages of cVAE for this problem:
        \begin{itemize}
            \item Multi-modality via latent variable $z$ (different $z$'s can map to different valid configurations on the circle) $\rightarrow$ addresses mode collapse relative to deterministic nets \textcolor{blue}{[assuming we avoid posterior collapse (decoder ignores $z$)]}
            \item Smooth generalization in $H$: theoretically, the encoder/decoder can learn continuous maps in $H$-space, allowing better interpolation than the nearest neighbors approach.
        \end{itemize}
        \item Note: $\theta$ is periodic, so training a Gaussian likelihood $p_\theta(Q \mid H, z)$ (in the decoder) on raw $\theta \in [0, 2\pi)$ is awkward because $\theta \approx 0$ and $\theta \approx 2\pi$ are close physically but far numerically ($\hat{\theta} = 2\pi - 0.01$ is a pretty good prediction for ground truth $\theta = 0.01$). To avoid this, instead of modeling $\theta \sim \mathcal{N}(\mu, \sigma^2)$, we can model a Gaussian over Cartesian coordinates in $\mathbb{R}^2$: \[(\cos \theta , \sin \theta) \sim \mathcal{N}(\mu_{cs}, \text{diag}(\sigma^2_{cs}))\]We can recover $\theta$ via $\mathrm{atan2}$ at inference time.
    \end{itemize}
    \item \textit{Invertible NN} (FrEIA): \textcolor{red}{[TODO: Read into math more]}, notes in \ref{inn}
    \begin{itemize}
        \item \textbf{Representation:} To avoid periodicity issues in $\theta$, we model: \[Q_{\text{feat}} = (x, y, \cos \theta) \in \mathbb{R}^4, \quad H = (h_x, h_y) \in \mathbb{R}^2\]
        \item \textbf{Conditional invertible map:} We learn an invertible mapping (for each condition $H$)
        \[f_\theta(\cdot; H): \mathbb{R}^4 \leftrightarrow \mathbb{R}^4, \quad z = f_\theta (Q; H), \quad Q = f_\theta^{-1} (z; H)\]
        with a simple base distribution $z \sim \mathcal{N}(0, I_4)$. In code this is implemented as a stack of conditional affine coupling blocks (FrEIA \texttt{SequenceINN} + \texttt{AllInOneBlock}) where each block receives $H$ as a conditioning input.
        \item \textbf{Training:} Using the change-of-variables formula, the conditional density is: 
        \[p_\theta(Q \mid H) = p_Z (f_\theta(Q; H)) \bigg|\det\frac{\partial f_\theta(Q; H)}{\partial Q}\bigg|\]
        Therefore:
        \[\log p_\theta(Q \mid H) = \log p_Z (z) + \log |\det J_{f_\theta}(x; H) |\]
        With $p_Z = \mathcal{N}(0, I)$, the per-example negative log-likelihood (dropping constants) is:
        \[\mathcal{L}_{\text{NLL}}(Q, H) = \frac{1}{2} ||z||^2 - \log |\det J_{f_\theta}(Q; H)|\]
        We minimize $\mathbb{E}_{(Q, H) \sim \text{data}} |\mathcal{L}_{\text{NLL}} (Q, H)|$.

        \item \textbf{Sampling:} At test time, for a given $H$ we sample:
        \[z \sim \mathcal{N}(0, I_4), \qquad Q = f_\theta^{-1}(z, H)\]
        then convert $Q = (x, y, \cos \theta, \sin \theta)$ back to $(x, y, \theta)$ with $\theta = \text{atan2} (\sin \theta, \cos \theta)$. 

        \item \textbf{Optional FK regularization (sample-space constraint):} Since the true conditional support lies on the IK manifold (all physically valid configurations for a given target must satisfy the IK constraint), we can add a penalty that directly enforces that generated samples reach $H$:
    \[
    \mathcal{L}
    =
    \mathbb{E}_{(Q,H)\sim\mathcal{D}}\big[\mathcal{L}_{\text{NLL}}(Q,H)\big]
    \;+\;
    \lambda_{\text{FK}}\,
    \mathbb{E}_{H\sim\mathcal{D},\, z\sim\mathcal{N}(0,I)}
    \Big[
      \|f_{\text{FK}}(f_\theta^{-1}(z;H)) - H\|^2
    \Big].
    \]
    In our implementation we compute FK error on inverse samples $\hat{Q} = f_\theta^{-1}(z;H)$ during training (rather than on the training $Q$ itself, as $\text{FK}(Q) = H$ by virtue of being in the dataset), so the regularizer shapes the \emph{sampled} conditional distribution.
    \end{itemize}

\end{enumerate}

\subsection{Evaluation} 
Assuming that the ground-truth $p^*(Q \mid H)$ is a uniform distribution over the feasible circle set, i.e.: $\theta \sim \text{Unif}[0, 2\pi) \rightarrow x = h_x - L\cos\theta, \, y = h_y - L\sin\theta$:
\begin{itemize}
    \item Accuracy: $e_{\text{hand}} (Q, H) \coloneq ||f(Q) - H||$
    \item Diversity/coverage: convert each sample to its implied angle on the circle: \[\theta^{(s)}_{\text{implied}} \coloneq \mathrm{atan2}\left(h_y - y^{(s)}, h_x - x^{(s)}\right)\]
    Check how well the empirical distribution over $\theta$ matches the target (here we assume $\theta \sim \text{Unif}[0, 2\pi)$): for histogram $\hat{p}(\theta)$, we can compute:
    \begin{itemize}
        \item KL divergence to uniform: $D_{\mathrm{KL}} (\hat{p}(\theta) \, ||\, \text{Unif})$
        \item Max angle gap: sort angles $\theta^{(s)}$ around the circle, compute max gap $\Delta_{\mathrm{max}} = \max_i (\theta_{i+1} - \theta_i)$
        
        Large $\Delta_{\text{max}}$ = model is missing big arcs (collapse)
    \end{itemize}
    \item ``Uses latent'' test (for cVAE/cINN): fix $H$, sample many $z$'s, then measure output variance $\mathrm{Var}(Q \mid H)$. If the model ignores $z$, this variance will be small.\\
    \textcolor{blue}{[$\downarrow$ TODO: Implement]}
    \item Generalization in $H$: make a test set where $H$ is (1) in-distribution, (2) near boundary, (3) out-of-distribution (slightly outside training workspace). Compare success and coverage!
    \item Inference speed per sample
\end{itemize}

\subsection{Extending to more general cases} Ideally, would be nice if this toy problem gave us an idea of the advantages/disadvantages of different approaches in regards to the following challenges:
\begin{itemize}
    \item \textit{Mode collapse:} Test: fixed $H$, sample 1k solultions, compute $\Delta_{\mathrm{max}}$, KL-to-uniform, etc.
    \item \textit{Generalization:} Test: distribution of $e_{\text{hand}} = || f(Q) - H||$
    \item \textit{Bias from data collection} (forward vs. inverse sampling): 
\end{itemize}

\subsection{Potential next-step extensions}
\begin{itemize}
    \item Disconnected feasible sets: modify toy with a ``visibility'' constraint such as \[\text{visible}(Q, H) = 1 \iff \theta \in [\alpha_1, \beta_1] \cup [\alpha_2, \beta_2]\]
    Now the true $p^*(Q \mid H)$ is two separated modes. \textbf{Test:} cluster sampled angles into arcs and compute mode recall: fraction of samples that land in each arc. 
    \item Scalability with dimension: add DoF to the toy (e.g., 2-link arm, variable stick length). \textbf{Test:} track how coverage/error degrades with dimension.
\end{itemize}

\section{Extra notes}

\subsection{Mode collapse}

A generative model has mode collapse if, for a fixed condition $H$, the samples $Q \sim \hat{p}(Q \mid H)$ cover only a small subset of the true support of $p^* (Q \mid H)$. 
\begin{itemize}
    \item In this toy problem, the support is the full circle parameterized by $\theta$. Mode collapse looks like the model always outputting $\theta \approx 0$ (i.e. stands in one favorite location) or outputting only a few discrete angles, ignoring the rest the circle, etc.
\end{itemize}

\subsection{Conditional VAE theory} \label{cvae} The goal is to model a multi-modal conditional distribution over robot configurations $Q$ given target $H$: $p^*(Q \mid H)$. In our simple robot setup, the true conditional has a 1D manifold of solutions (a circle in $(x,y)$ paired with corresponding $\theta$), so it's not unimodal in the usual sense (rather than being a single blob in $\mathbb{R}^d$, for fixed $H$, probability mass $p^*(Q \mid H)$ is concentrated along a ring in $(x,y)$ with a corresponding orientation $\theta$ at each point). 

\subsubsection{Motivation} Because $p^*(Q \mid H)$ is not unimodal, if we model $p(Q \mid H)$ as a single Gaussian/predict a single mean, we face mean collapse: the mean of points around a circle is the center of the circle, but the center is invalid. So the ``best'' unimodal prediction is often physically invalid/low-probability under the true distribution.

\subsubsection{Model architecture} We introduce latent $z \in \mathbb{R}^{d_z}$, which ideally will be the variable that ``chooses'' \textit{which} solution the model means among the many possible solutions on the ring for a given $H$. Define a prior $p(z) = \mathcal{N}(0, I)$. There are two parts to the model (decoder and encoder):
\begin{itemize}
    \item \underline{Decoder (generative model):} The decoder is a conditional distribution $p_\theta(Q \mid H, z)$. For the architecture of the decoder, we use the diagonal Gaussian \textcolor{blue}{(potential next step -- try other architectures)}: \[p_\theta(Q \mid H, z) = \mathcal{N}(Q; \mu_\theta(H, z), \text{diag}(\sigma^2_\theta (H, z)))\] where $\mu_\theta(H,z)$ and $\sigma^2_\theta$ are the predicted mean/variance of each coordinate of $Q$. 
    \item \emph{Interlude:} $p_\theta(Q, z \mid H) = p(z \mid H) p(Q \mid z, H) = p(z) p_\theta(Q \mid H, z)$ [assuming $p(z\mid H) = p(z)$]. Bayes' rule gives:
    \[p_\theta(z\mid Q, H) = \frac{p_\theta(Q, z \mid H)}{p_\theta(Q \mid H)} = \frac{p(z) p_\theta (Q \mid H, z)}{p_\theta(Q \mid H)}\]
    Expanding the denominator: $p_\theta(Q \mid H) = \int p(z') p_\theta (Q \mid H, z') \, dz'$ (consider all possible latent explanations $z'$, weight how likely each would produce $Q$, then sum them up). Plugging this in,
    \[p_\theta(z\mid Q, H) =\frac{p(z) p_\theta(Q \mid H, z)}{\int p(z') p_\theta(Q \mid H, z') \, dz'}\]
    The denominator $\int p(z') p_\theta(Q \mid H, z') \, dz'$ is intractable: $p_\theta(Q \mid H, z') = \mathcal{N}(Q; \mu_\theta(H, z'), \Sigma_\theta(H, z'))$, so we effectively have
    \[p(z') p_\theta(Q \mid H, z') \Rightarrow \mathcal{N}(z'; 0, 1) \times \mathcal{N}(Q; \mu_\theta(H, z'), \Sigma_\theta(H, z'))\]
    For fixed $Q$ and $H$, $\mathcal{N}(Q; \mu_\theta(H, z'), \Sigma_\theta(H, z'))$ becomes some complicated nonnegative function of $z'$ $\rightarrow g(z')$. Then, 
    \[p_\theta(Q \mid H) = \int \mathcal{N}(z'; 0, I) g(z')\, dz'\]
    This is very hard to compute analytically: recall the multivariate Gaussian density is given by:
    \[f_{\mu, \Sigma}(Q) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(- \tfrac12 (Q-\mu)^\top \Sigma^{-1} (Q-\mu)\right)\]
    Here, we plug in $\mu = \mu_\theta (H, z'), \Sigma = \Sigma_\theta (H, z')$. These are neural-net outputs, i.e. complicated nonlinear functions of $z'$, which makes the integral difficult to compute. 

    Since the true posterior is intractable, introduce a tractable approximation $q_\phi(z \mid Q, H) : (Q, H) \mapsto p(z)$ which essentially answers ``If this $Q$ came from some latent $z$, what must that $z$ have been?''

    \item \underline{Encoder (inference model):}  The encoder is the conditional distribution $q_\phi(z \mid Q, H)$. 
\end{itemize}

% \subsubsection{ELBO loss}

\subsection{FrEIA INN theory} \label{inn} The goal is to learn $p(Q \mid H)$, where $Q = (x, y, \cos \theta, \sin \theta) \in \mathbb{R}^4$ (equivalent form of $(x, y, \theta)$) and $H = (h_x, h_y) \in \mathbb{R}^2$. A normalizing flow models an invertible map (for each fixed condition $h$): 
\[f_\theta(\cdot; h) : \mathbb{R}^4 \rightarrow \mathbb{R}^4, \quad z = f_\theta (q; h), \quad q = f_\theta^{-1}(z; h)\]
We pick the base density for $z$ to be the multivariate standard normal: \[p_Z(z) = \mathcal{N}(0, I_4)\]
Then, $p_\theta(q \mid h)$ can be defined via ``push $q$ through $f$ to get $z$, and measure how likely that $z$ is, corrected by volume change'' $\rightarrow$ correction is the Jacobian determinant. 

\subsubsection{Hand-wavy intuition} For each condition $h$, the true data $q$ lives in some weird, potentially multi-modal shape (ring of solutions in our simplified problem). A Gaussian cannot represent that shape directly.
\begin{itemize}
    \item \textit{Core idea:} Instead of trying to fit a complicated distribution directly, we learn a warp of space that makes the complicated shape look Gaussian.
    \begin{itemize}
        \item Forward direction: $z = f_\theta(q; h)$ takes real samples $q$ and maps them into ``Gaussian space''
        \item Training enforces: ``after mapping, these $z$'s should look like samples from $\mathcal{N}(0, I)$''
        \item Warping space changes volume $\rightarrow$ probability densities change:
        \begin{itemize}
            \item If $f$ expands a region of $q$-space, then points there become less dense in $q$-space. 
            \item If $f$ compresses a region, points become more dense.
        \end{itemize}
    \end{itemize}
\end{itemize}
After learning an invertible NN model, we can generate realistic $q$ by:
\begin{enumerate}
    \item Sample $z \sim \mathcal{N}(0, I)$
    \item Invert: $q = f_\theta^{-1}(z; h)$
\end{enumerate}

\textit{Fun way of thinking about it:} Imagine data for $q$'s is shaped like a bent pretzel in $\mathbb{R}^D$. A flow learns a continuous, invertible deformation of space that morphs the pretzel into a $D$-dim. Gaussian blob. Forward pass = ``unbend the pretzel into a Gaussian.'' Inverse pass = ``bend a Gaussian blob back into a pretzel.'' The Jacobian term is bookkeeping of how the deformation stretches space.

\subsubsection{Why layers need to be invertible}
\begin{enumerate}
    \item Invertible layers allow flow models to compute an exact probability for any observed training point: if $Q$ is a real solution from the dataset for some $H$, we can compute exactly how likely the model thinks it is (flows are the only model out of GANs, VAEs, and diffusion models that can compute the exact log-likelihood of a new sample)
    \[\Pr(\mathbf{f} (\mathbf{Q})\mid \mathbf{H}, \mathbf{\phi}) = \bigg|\frac{\partial \mathbf{f} (\mathbf{z}, \mathbf{H}, \phi)}{\partial \mathbf{z}}\bigg|^{-1} \cdot \Pr(\mathbf{z}) \text{ where } \mathbf{z} = \mathbf{f}^{-1} (\mathbf{x}, \mathbf{H}, \phi) \text{ and } \phi = \text{learned model params}\]
    \item Invertibility allows sampling to be one inverse pass rather than iterative denoising, sampling chains, etc.: once trained, we can directly (1) sample $z \sim \mathcal{N}(0, I)$ (2) compute $Q = f_{\theta}^{-1} (z; H)$
\end{enumerate}

\subsubsection{More concrete math} In separate PDF (\texttt{flow model FrEIA implementation notes.pdf}).


\subsection{Normalizing flow} In separate PDF (\texttt{normalizing flow notes.pdf}).


\subsection{Conceptual comparison of different methods}

Given the data is shaped like a pretzel in $\mathbb{R}^D$ \ldots

\subsubsection{Flow} Learn an invertible deformation conditioned on $H$ that straightens the pretzel into a Gaussian $\mathcal{N}(0, I_D)$. Sampling works by running the inverse deformation on $z \sim \mathcal{N}(0, I_D)$ and $H$ to obtain $Q^*$. 

\subsubsection{cVAE} Learn a conditional generator that uses Gaussian knobs to choose how to output the pretzel; training teaches the knobs to stay Gaussian while reconstructing data.

\subsubsection{Diffusion} Instead of unbending the pretzel into a Gaussian in one shot, diffusion dissolves the pretzel into pure noise by gradually adding noise, then learns to sculpt noise back into the pretzel step-by-step.

\subsection{Invertible neural networks vs. normalizing flow}

Invertible neural networks are a class of deep networks that approximate bijective functions and are characterized by a forward mapping that can be inverted. All normalizing flow networks are invertible neural networks. 

\subsubsection{INNs as generative models} Because INNs learn a transformation between the input data distribution and a simple prior distribution (typically a normal distribution), the inverse mapping automatically acts as a generator, converting the simple distribution into samples resembling the input data. 

\subsubsection{INNs that are not normalizing flow models} Invertible neural networks that are not flow models exist \textcolor{purple}{(TODO: Read into this paper \cite{INN2019})}.

% All normalizing flow networks are invertible neural networks! Here is the math behind why:


\pagebreak
\bibliographystyle{plain} % or another style you prefer
\bibliography{reachability_references}       % Note: No need to include the .bib extension

\end{document}
