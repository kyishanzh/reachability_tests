\documentclass{amsart}
\usepackage[margin=1in]{geometry}

\usepackage{comment}
\usepackage{amsthm, amsmath, amssymb}
\usepackage[colorlinks]{hyperref}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{tikz}
\usetikzlibrary{calc,matrix,arrows,decorations.markings}
\usepackage{array}
\usepackage{color}
\usepackage{enumerate}
\usepackage{nicefrac}
\usepackage{listings}
\bibliographystyle{plainurl}
\usepackage{cite}
\usepackage{mathtools}
\usepackage{enumitem}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\lstset{
    basicstyle=\small\ttfamily,
    keywordstyle=\color{blue},
    language=python,
    xleftmargin=16pt,
}

\topmargin=-0.5in
\headheight=0in
\pagestyle{plain}


% ------   Theorem Styles -------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{fact}[theorem]{Fact}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{setup}[theorem]{Setup}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\setlength{\parindent}{0pt}



\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\abss}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\snorm}[1]{\lVert#1\rVert}
\newcommand{\ang}[1]{\left\langle #1 \right\rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\sqb}[1]{\left[ #1 \right]}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\setcond}[2]{\left\{ #1 \;\middle\vert\; #2 \right\}}
\newcommand{\cond}[2]{\left( #1 \;\middle\vert\; #2 \right)}
\newcommand{\sqcond}[2]{\left[ #1 \;\middle\vert\; #2 \right]}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}

\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\raggedbottom

\title{Reachability Project Log :)}
\author[]{}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\section{Problem Description}

Our goal is to learn \textit{where the robot should stand} given a desired grasp. 

\subsection{Problem inputs} Formulating the problem, we are given as inputs:
\begin{itemize}
    \item $x_e \in SE(3)$: desired end-effector pose in world coordinates
    \item $b \in SE(2)$: wheeled mobile base pose $(x, y, \gamma)$
    \item $q \in \mathbb{R}^n$: arm joint configuration.
\end{itemize}

\subsection{Problem outputs} We want to find a range of feasible whole-body configurations that satisfy the following requirements:
\begin{enumerate}
    \item \textbf{IK feasibility:} $\text{forward kinematics}(b, q) = x_e$
    \item \textbf{Visibility/sensing constraints:} The end-effector should lie within the camera's field of view and range (and un-occluded by the robot itself), i.e. $\text{visible}(b, q, x_e) = 1$
    \item \textbf{Ball of feasible configurations:} We want a region in joint space around $(b,q)$ that keeps the hand in approximately the right pose and satisfies the above constraints. 
\end{enumerate}

Our target is to model some distribution $p(b, q\mid x_e)$ subject to these constraints $\uparrow$, ideally with some notion of pose quality (visibility, manipulability, clearance, etc.). 

% \section{3D robot v2: LeKiwi robot}

\section{2D robot v3: Point $(x,y, \psi)$ base with a rotary 3-link arm}

\subsection{Setup}
\begin{itemize}
    \item Robot base configuration: $(x, y, \psi)$ where $\psi \in [0, 2\pi)$ is some arbitrary base heading (robot does \textit{not} need to be facing the target)
    \item Robot arm configuration: $(\theta_1, \theta_2, \theta_3)$
    \item Target pose specification: $(h_x, h_y, \phi)$ (vector with tip at $(h_x, h_y)$ and heading $\phi$)
\end{itemize}

Complete robot configuration specification:
\[(x, y, \psi, \theta_1, \theta_2, \theta_3) \in \mathbb{R}^6\]


\subsubsection{Data generation pipeline}

\begin{center}
    \textcolor{cyan}{\textsc{The Math}}
\end{center}
The goal is to generate a data sampling pipeline for pairs $(q, h)$ where:
\[q = (x, y, \theta_1, \theta_2, \theta_3) \in \mathbb{R}^6 \qquad h = (h_x, h_y, \phi) \in \mathbb{R}^3\]

\textbf{State space definitions:}
\begin{itemize}
    \item Base configuration: $(x_B, y_B, \psi) \in SE(2)$
    \item Arm configuration: $\theta = (\theta_1, \theta_2, \theta_3)\in \mathbb{R}^3$
    \item Target pose: $H = (h_x, h_y, \phi) \in SE(2)$
\end{itemize}

\textbf{Notation and coordinate frames:}
\begin{itemize}
    \item Frames:
    \begin{itemize}
        \item Frame $W$: The fixed world frame
    \item Frame $B$: The robot base frame
    \item Frame $E$: The robot end-effector frame (tip of the arm)
    \item Frame $H$: The target hand frame (where we want the end-effector to be)
    \end{itemize}

    \item Variables:
    \begin{itemize}
        \item $^A p^C_A$: Position vector from origin of $A$ to origin of $C$, expressed in frame $A$.
        \item $^A R^B$: Rotation matrix describing orientation of $B$ relative to $A$
        \item $T(p, \alpha)$: Planar homogeneous transform for position $p \in \mathbb{R}^2$ and heading $\alpha$
        \[T(p, \alpha) = \begin{bmatrix}
    \cos \alpha & - \sin \alpha & p_x \\ \sin \alpha & \cos \alpha & p_y \\ 0 & 0 & 1
\end{bmatrix} = \begin{bmatrix}
    R(\alpha) & p \\ 0 & 1
\end{bmatrix}\]Transform inverse:
\[T(p, \alpha)^{-1} = \begin{bmatrix}
    R(\alpha)^\top & - R(\alpha)^\top p \\ 0 \quad 0 & 1
\end{bmatrix}\]
\textcolor{purple}{$\uparrow$ Work out derivation for this inverse formula later!}
    \end{itemize}
\end{itemize}

% Defining some notation:
% \begin{itemize}
%     \item $^A p^C$: Position of $C$ measured from $A$ (vector $\vec{AC}$ = tail at $A$, head at $C$), measured from $A$'s origin $A_0$. We assume this position vector is expressed in $A$'s basis.
%     \item $^B R^A$: Orientation of frame $A$ relative to frame $B$
%     \item $^A T^B$: Pose of frame $B$ in frame $A$
%     \begin{itemize}
%         \item Pose = position (location) + orientation of a frame $B$ with respect to another frame $A$
%         \item Transform = linear operator that maps a point whose location is known in frame $B$ to that same point's location in frame $A$. 
%     \end{itemize}
% \end{itemize}

\textbf{The kinematic constraint (our goal):} Using the introduced notation, we define the frame of the base in the world as:\[^W T^B = T\left(\begin{bmatrix}
    x \\ y
\end{bmatrix}, \psi\right)\]

We wish to find a base pose such that the end-effector $E$ perfectly aligns with a sampled target $H$:
\[^W T^E = {^W T^H}\]
Expanding the kinematic chain from [World $\rightarrow$ Base $\rightarrow$ End-effector]:
\[^W T^B \cdot {^B T^E}(\theta) = {^W T^H}\]
Our goal is to find $^W T^B$ that makes the above constraint true. Solving for the base pose:
\[{^W T^B} = {^W T^H} \left[{^B T^E}(\theta)\right]^{-1}\]

Using the transform inverse property $[{^A T^B}]^{-1}= {^B T^A}$, this becomes:\[^W T^B = {^W T^H} \cdot {^E T^B}(\theta)\]

Let us now expand the matrix multiplication:
\begin{align*}^W T^B = {^W T^H} \cdot {^E T^B}(\theta) \Longrightarrow \begin{bmatrix}
    {^W R^B} & {^W p^B_W} \\ 0 & 1
\end{bmatrix} &= \begin{bmatrix}
    {^W R^H} & {^W p^H_W} \\ 0 & 1
\end{bmatrix} \begin{bmatrix}
    {^E R^B} & {^E p^B_E} \\ 0 & 1
\end{bmatrix} \\
&= \begin{bmatrix}
    {^W R^H} \cdot {^E R^B} & {^W R^H} \cdot {^E p^B_E} + {^W p_W^H} \\ 0 & 1
\end{bmatrix}\end{align*}

Plugging in our numbers:
\begin{itemize}
    \item \textbf{Orientation:} ${^W R^B} = {^W R^H} \cdot {^E R^B} \Longrightarrow$ in scalar angles, $\psi = \phi - (\theta_1 + \theta_2 + \theta_3)$ \textcolor{purple}{(Work out the math here more carefully later.)}
    \item \textbf{Position:} ${^W p^B_W}= {^W R^H} \cdot {^E p^B_E} + {^W p_W^H}$
    \begin{itemize}
        \item[$\star$] ${^W p_W^H}$ is the sampled target position $[h_x, h_y]^\top$
        \item[$\star$] ${^E p^B_E}$ is the position of the base relative to the tip, expressed in the tip frame. 
        \begin{itemize}
            \item $\uparrow$ We can rewrite ${^E p^B_E}$ using the known forward kinematics vector $^B p^E_B$ (Base $\rightarrow$ Tip): $^B p^E_E = -{^E R^B} {^B p^E_B}$
            \begin{proof}[Derivation] We start wtih ${^B p^E_B}$, which is the vector pointing from base to end-effector in the base frame. Physically, this is the exact opposite vector to the vector pointing from end-effector to base in the base frame, i.e. ${^B p^E_B} = -{^E p^B_B}$. However, the term we are solving for is $^B p^E_E$, so we need to do a change of basis:
                \[\underbrace{^E p^B_E}_\text{Tip to base in frame $E$} = \underbrace{^E R^B}_\text{\text{Rotate $B$ to $E$}} \cdot \left(\underbrace{- ^B p^E_B}_{\text{Tip to base in $B$}}\right)\]
                
            \end{proof}
        \end{itemize}
        Simplifying via rotation composition ($^W R^H {^E R ^B} = {^W R^B}$ assuming $E=H$):
        \[{^W p^B_W}= -{^W R^B} \cdot {^B p^E_B} + {^W p_W^H}\]
        $\uparrow$ Geometric intuition: The base is located at the target position, minus the arm's reach vector rotated by the base's global heading.
    \end{itemize}
\end{itemize}

\begin{comment}
Using forward kinematics, for a 3-link planar arm with link lengths $L_1, L_2, L_3 > 0$ and cumulative angles $\alpha_1 = \theta_1, \alpha_2 = \theta_1 + \theta_2, \alpha_3 = \theta_1 + \theta_2 + \theta_3$, we have the following end-effector position in the base frame:
\[^B p^E(\theta) = \begin{bmatrix}
    L_1 \cos \alpha_1 + L_2 \cos \alpha_2 + L_3 \cos \alpha_3 \\
    L_1 \sin \alpha_1 + L_2 \sin \alpha_2 + L_3 \sin \alpha_3
\end{bmatrix}\]
The end-effector heading in base frame is:\[^B R^E = {^B \phi^E}(\theta) = \alpha_3\]
So the base-to-EE transform is:
\[^B T^E(\theta) = T({^B p^E}(\theta), {^B R^E} (\theta)) = {^B R^E} {^E T^E} + ^B p^E(\theta)\]

World end-effector transform (forward kinematics):
\[^W T^E = {^W T^B} {^B T ^E}(\theta)\]

The target label is the end-effector pose:
\[H = (h_x, h_y, \phi) \iff {^W T ^H} = T\left(\begin{bmatrix}
    h_x \\h_y
\end{bmatrix}, \phi\right)\]

The kinematic constraint that the end-effector pose = target pose can be written as:
\[^W T^H = {^W T ^E} = {^W T^B} {^B T ^E}(\theta)\]
Solving for the base pose:
\[{^W T^B} = {^W T^H} \left[{^B T^E}(\theta)\right]^{-1}\]
So, to generate $(Q, H)$ data points, we can sample $\theta$, sample a target $H$, and compute the base pose by one matrix inverse. 
\vspace{1mm}
Math-ing out the above: Let $p_H = \begin{bmatrix}
    h_x \\ h_y
\end{bmatrix}$, and define:
\[^B p^E = {^B p^E}(\theta), \quad {^B \phi^E} = {^B \phi^E}(\theta) = \theta_1 + \theta_2 + \theta_3\]

Since $T_{BE}^{-1}$ has rotation $-\phi_{BE}$ and translation $-R(\phi_{BE})^\top p_{BE}$, composing gives:
\[\text{Base heading: }\psi = \phi - \phi_{BE} \qquad \text{Base position: }\begin{bmatrix}
    x \\ y
\end{bmatrix} = p_H - R(\psi)p_{BE}\]
So given $(H = [h_x, h_y, \phi], \theta = [\theta_1, \theta_2, \theta_3])$, base pose $(x,y, \psi)$ is uniquely determined. 
\end{comment}

\begin{center}
    \textcolor{cyan}{\textsc{The Sampling Pipeline}} \\
    (Replace 3 with $N$ in the below math to extend the pipeline to an arbitrary number of links)
\end{center}

Inputs/hyperparameters:
\begin{itemize}
    \item Link lengths $L_1, L_2, L_3$
    \item Joint distributions with limits: \[\theta_1 \sim \text{Unif}[a_1, b_1], \theta_2 \sim \text{Unif}[a_2, b_2], \theta_3 \sim \text{Unif}[a_3, b_3] \Leftarrow \mathcal{D}_\theta \]
    \begin{itemize}
        \item Can also have multimodal joint distributions: e.g., $\theta_1 \sim \text{Unif}[[a_1, b_1] \cup [c_1, d_1]]$
    \end{itemize}
    \item Target sampling distribution: we will assume uniform end-effector position and heading across the workspace \[h_x, h_y \sim \text{Unif}([-w, w]^2), \quad \phi \sim \text{Unif}[0, 2\pi)\Leftarrow \mathcal{D}_H\]
\end{itemize}

Pipeline:
\begin{enumerate}
    \item \textbf{Sample joint configuration:} $\theta = (\theta_1, \theta_2, \theta_3) \sim \mathcal{D}_\theta$
    \item \textbf{Compute arm FK in base frame:}\[{^B p^E_B}(\theta) = \begin{bmatrix}
        \sum_{i=1}^3 L_i \cos \left(\sum_{j=1}^i \theta_j\right) \\
        \sum_{i=1}^3 L_i \sin \left(\sum_{j=1}^i \theta_j\right)
    \end{bmatrix}, \qquad \phi_{\text{arm}}(\theta) = \sum_{i=1}^3 \theta_i \]
    \item \textbf{Sample a target pose:} $H = (h_x, h_y, \phi) \sim \mathcal{D}_h$. 
    \item \textbf{Analytic base solve:}\[\boxed{\psi = \text{wrap}_{\pi} (\phi - \phi_{\text{arm}})}, \quad \begin{bmatrix}
        x_B \\y_B
    \end{bmatrix} = \begin{bmatrix}
    h_x \\ h_y
    \end{bmatrix} - R(\psi) {^B p^E_B} = \boxed{\begin{bmatrix}
    h_x \\ h_y
    \end{bmatrix} - \begin{bmatrix}
    \cos \psi & - \sin \psi \\ \sin \psi & \cos \psi
    \end{bmatrix} {^B p^E_B}}\]
    \item \textbf{Rejection sampling (validity checks):} The analytic solution guarantees the end-effector reaches the target, but not that the configuration is valid. We run a rejection sampling loop that checks for:
    \begin{itemize}
        \item Self-collision: Check if any links intersect with each other. \textcolor{blue}{[To add: Check if any links intersect with the base point.]}
        \item \textcolor{blue}{[To add: Environment-collision/obstacles]}
    \end{itemize}
    If check passes: Store the pair $(Q, H)$ in the dataset where $Q = (x_B, y_B, \psi, \theta_1, \theta_2, \theta_3)$. If check fails, discard $(Q,H)$ and return to Step 1. 
\end{enumerate}
$\uparrow$ Repeat until you have $N$ samples / reach max retries for rejection sampling. 
\vspace{1mm}

TL;DR: For $i = 1, \ldots, N$, we sample from the joint $(\theta, H)\sim \mathcal{D}_\theta \times \mathcal{D}_H$:
\begin{enumerate}
    \item Sample $\theta^{(i)} \sim \mathcal{D}_\theta$
    \item Sample $H^{(i)} \sim \mathcal{D}_H$ 
    \item Deterministically compute $Q^{(i)} = f(\theta^{(i)}, H^{(i)})$
    \item Run rejection sampling loop to ensure that the $(Q,H)$ computed satisfies specified constraints before adding it to the dataset.
\end{enumerate}
\subsubsection{Featurization}
Defining $R = L_1 + L_2 + L_3$ (robot reach),
\begin{align*}
    Q &= (x, y, \psi, \theta_1, \theta_2, \theta_3)  \mapsto \\
    &\boxed{Q_{\text{feat}} = \left(\frac{\tilde{\Delta}_x}{R}, \frac{\tilde{\Delta}_y}{R}, \cos(\tilde{\psi}), \sin(\tilde{\psi}), \cos(\theta_1), \sin(\theta_1), \cos(\theta_2), \sin(\theta_2), \cos(\theta_3), \sin(\theta_3)\right)} \in \mathbb{R}^{10}\\
H &= (h_x, h_y, \phi), C = (\ldots) \mapsto \boxed{C_\text{feat} = (0, 0)}  \in \mathbb{R}^2
\end{align*}
where
\[
    \tilde{\Delta} = R(-\phi) \cdot \begin{bmatrix}
        x - h_x \\ y - h_y 
    \end{bmatrix} \qquad
    \tilde{\psi} = \psi - \phi
\]

The featurization here canonicalizes the target: we take the robot's world-frame coordinates $(x,y, \psi)$ and re-describe them from the target-frame. 
\[\Delta_\text{world} = \begin{bmatrix}
    x - h_x \\ y - h_y
\end{bmatrix}\]

This vector tells us how far the robot is from the target in the orientation of the room. To align the world with the target's heading, we multiply $\Delta_\text{world}$ with the inverse of the rotation matrix of the target's heading: this way, the target's heading becomes the new $0^\circ$. 

\begin{center}
    \includegraphics[scale=0.5]{diagrams/q_feat canonicalization fig.png}
\end{center}

In addition, we featurize $\tilde{\psi} = \psi - \phi$ to extract the relative angle between the robot base heading and the target heading: this relative angle is what matters to the model when deciding reachability (the compass of the world frame does not affect the relative relationship between robot and target). We see this transformation in the figure below:
\begin{center}
    \includegraphics[scale=0.4]{diagrams/q_feat canonicalization psi.png}
\end{center}

\subsection{Evaluation metrics}

\subsubsection{Fidelity metrics} \textcolor{brown}{Does the robot reach the target?}

\begin{itemize}
    \item \textbf{Positional error (RMSE):} Euclidean distance:
    \[E_\text{pos} (\hat{q}, h) = ||\text{FK}_{\text{pos}}(\hat{q}) - h_{\text{pos}}||\]
    \item \textbf{Rotational error (Geodesic):} Angular distance on circle between $\text{FK}_{\text{ori}}(\hat{q})$ and $h_\text{ori}$ (the shortest arc length on the unit circle $S^1$ between the robot's end-effector heading and the target heading):
    \[E_{\text{ori}}(\hat{q}, h) = d_{S^1} (\text{FK}_\text{ori}(\hat{q}), h_\phi)\]
    \begin{itemize}
        \item Derivation and proof: \textcolor{purple}{Look into more detail later! We implement in the code as:} \[d_{S^1}(\theta_1, \theta_2) = |\text{atan2}(\sin (\theta_1 - \theta_2), \cos(\theta_1, \theta_2))|\]
        This guarantees the error is always in $[0, \pi]$
    \end{itemize}
\end{itemize}

\subsubsection{Feasibility metrics} \textcolor{brown}{Is the robot broken?}

\begin{itemize}
    \item \textbf{Self-collision rate:} Percentage of generated samples that fail \texttt{env.check\_self\_collision}
    \[R_{\text{coll}} = \frac{1}{N} \sum_{i=1}^N \mathbf{1}(\text{CheckColl}(\hat{q}_i))\] where $\mathbf{1}(\cdot)$ is the indicator function.
    \begin{itemize}
        \item This is the probability that a generated sample intersects itself.
        \item Interpretation: If $R_{\text{coll}} > 0$, the model is hallucinating configurations that contain self-collisions.
    \end{itemize}
    \item \textbf{Joint limit violation rate:} Percentage of generated samples where any $\theta_i$ is outside the specified joint limits (when generating training data). \[R_{\text{limit}} = \frac{1}{N} \sum_{i=1}^N \mathbf{1} \left(\exists j : \hat{q}_{i,j} \notin \left[L_{\text{min}}^j, L_{\text{max}}^j\right]\right)\]
\end{itemize}

\subsubsection{Distribution metrics} \textcolor{brown}{Does the model capture the full valid solution space?}
\begin{itemize}
    \item \textbf{MMD (Joint space):} Kernel distance between model samples and ground truth samples on full state $q$ (measures global distribution match). Outputs a single scalar score representing the distance between the model distribution $M$ and ground truth distribution $G$.
    \begin{itemize}
        \item Mathematical derivation: We map distributions to a Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}$ via a feature map $\phi(x)$. If the ``mean embeddings'' (centers of mass) in this space are identical, the distributions are identical:
        \[\text{MMD}^2(M, G) = ||\mu_M - \mu_G||_{\mathcal{H}}^2\]
        Expanding the norm via the inner product $\langle \cdot \rangle_{\mathcal{H}}$: \begin{align*}
            \text{MMD}^2 &= \langle \mu_M - \mu_G, \mu_M - \mu_G \rangle \\ &= \langle \mu_M, \mu_M \rangle + \langle \mu_G, \mu_G \rangle - 2 \langle \mu_M, \mu_G \rangle
        \end{align*}
        Using the ``Kernel Trick'' $k(x,y) = \langle \phi(x), \phi(y)\rangle$, we avoid computing the infinite-dimensional $\phi$ and simply compute pairwise similarities: \[\text{MMD}^2 \approx \underbrace{\frac{1}{(N_M)^2} \sum_{i,j} k(x_i, x_j) + \frac{1}{(N_G)^2} \sum_{i,j} k(y_i, y_j)}_{\text{Self-similarity (clumping)}} - \underbrace{\frac{2}{N_M \cdot N_G}\sum_{i,j}k(x_i, y_j)}_{\text{Cross-similarity}}\]
        To minimize MMD, the model samples $x$ must be as close to ground truth samples $y$ as they are to each other (MMD $\approx 0 $ only when samples within $M$ look similar to each other, samples within $G$ look similar to each other, and samples across $P$ and $Q$ look similarly similar).

        \item Caveat: MMD is sensitive to kernel bandwidth. Too small $\rightarrow$ only near-exact matches matter; too large $\rightarrow$ everything looks similar and MMD loses power. We implement multi-kernel MMD (mean over multiple kernel bandwidths) to address this caveat.
    \end{itemize}
    \item \textbf{EMD (Target bearing):} Wasserstein distance between model bearing angles and ground truth bearing angles (measures if the robot is approaching from the correct valid directions).
    \begin{itemize}
        \item Intuition: EMD measures the minimum ``work'' required to transform one distribution of mass into another, where ``work'' = (amount of mass moved) $\times$ (distance moved). \textcolor{gray}{(Example: Imagine $M$ is a pile of dirt spread along a line, and $G$ is the shape you want. You're allowed to shovel dirt around. $W_1$ is the cheapest total shoveling cost.)}
        \item For this metric, we define:
        \begin{itemize}
            \item $M$ = distribution of bearing angles produced by the model (angle of vector from robot base to the target, measured in the world frame)
            \item $G$ = distribution of bearing angles from ground truth
        \end{itemize}
        \item Math: \[W_1 (M, G) = \inf_{\gamma \in \Pi(M, G)} \mathbb{E}_{(x,y) \sim \gamma}[|x-y|]\]
        For 1D distributions, this has a closed-form solution using the inverse CDF. If we sort samples $x_{(1)}\leq \cdots \leq x_{(N)}$ and $y_{(1)}\leq \cdots \leq y_{(N)}$: \[W_1 \approx \frac{1}{N} \sum_{i=1}^N |x_{(i)} - y_{(i)}|\]
        \item Nuance for circles: Since bearing is periodic $(0 \approx 2\pi)$, standard linear sorting fails. We can either (a) assume the distribution is unimodal and unroll it, or (b) use a circular transport solver. \textcolor{blue}{Figure out which approach to take!}
    \end{itemize}
\end{itemize}

\subsubsection{Coverage \& fidelity (Manifold precision/recall)} \textcolor{brown}{Goal: Explicitly measure mode collapse vs. hallucination.}

\begin{itemize}
    \item \textbf{Coverage (Recall):} For every real valid solution, is there a generated solution nearby?\[\text{Coverage}(Q_\text{ground truth (gt)}, Q_{\text{model}}) = \frac{1}{|Q_{\text{gt}}|} \sum_{y \in Q_{\text{gt}}} \min_{x \in Q_{\text{model}}} || y - x||_2\]
    \begin{itemize}
        \item This measure the average distance from a ground truth sample to its nearest generated neighbor.
        \item Interpretation: If this is high, the model has \textit{mode collapse} $\rightarrow$ there are valid regions of the solution space (ground truth) that the model is ignoring (no samples near them).
    \end{itemize}
    \item \textbf{Fidelity (Precision):} For every generated solution, is there a real valid solution nearby?\[\text{Fidelity}(Q_{\text{model}}, Q_\text{ground truth (gt)}) = \frac{1}{|Q_{\text{model}}|} \sum_{x \in Q_{\text{model}}} \min_{y \in Q_{\text{gt}}} || x-y||_2\]
    \begin{itemize}
        \item This measure the average distance from a generated sample to the manifold of valid ground truth samples. 
        \item Interpretation: If this is high, the model is \textit{hallucinating} $\rightarrow$ it is generating samples in regions of the configuration space that are actually invalid/impossible according to the ground truth sampler.
    \end{itemize}
\end{itemize}

% \begin{itemize}
%     \item FK hand error: position and orientation
%     \item Maximum mean discrepancy (MMD): TL;DR, MMD is defined by the idea of representing distances between distributions as distances between mean embeddings of features. 
%     \item EMD:
% \end{itemize}


\section{Simplified problem v2.1: Round thing with a rotary 2-link arm, generalizable to different workspace sizes}

\textcolor{blue}{\textbf{Context:} The previous setup specified in section \ref{v2.0} learns a model that is sensitive to workspace size.} We fix this problem in this new training setup.

\subsection{Setup} \label{v2.1 setup}

\begin{itemize}
    \item\colorbox{lime!30}{Data generation pipeline:} Same as the data generation pipeline described in section \ref{v2.0 setup}.
    \item \colorbox{pink!30}{Modeling assumptions:}
    \begin{itemize}
        \item Robot heading should face the target $H$
    \end{itemize}
    \item \colorbox{cyan!20}{Featurization:} defining $R = L_1 + L_2$ (robot reach),
    \[Q = (x, y, \psi, \theta_1, \theta_2)  \mapsto \boxed{Q_{\text{feat}} = \left(\frac{x - h_x}{R}, \frac{y - h_y}{R}, \cos(\psi), \sin(\psi), \cos(\theta_1), \sin(\theta_1), \cos(\theta_2), \sin(\theta_2)\right)}\]
    \[H = (h_x, h_y), C = (\ldots) \mapsto \boxed{C_\text{feat} = (0, 0)} \]
    The feature vector $H_{\text{feat}}$ is no longer necessary because $H$ has been encoded into our featurization for $(x,y)$ in $Q_{\text{feat}}$, but we keep it around in the code with \texttt{dCond = 0} in case we want to add conditioning features later (e.g., conditioning on pictures of the environment, obstacles, terrain, etc.).
    \item \colorbox{violet!15}{Model learning setup:}
    \begin{itemize}
        \item Input: $Q_{\text{feat}}$ (+ empty conditioning vector $C_\text{feat}$ )
        \item Goal: Model learns distribution $\boxed{p(Q_\text{feat} \mid C_\text{feat})}$ that maximizes the predicted probability of seeing the ground truth data distribution $(Q_{\text{feat}}, C_\text{feat})$ pairs.
    \end{itemize}
    \item Things that the model should generalize across:
    \begin{itemize}
        \item The learned model should be workspace invariant:
    \end{itemize}
    \item Things that the model will not be capable of generalizing across:
    \begin{itemize}
        \item Hardware parameters:
        \begin{itemize}
            \item Link lengths (\texttt{link\_lengths})
            \item Joint limits (\texttt{joint\_limits})
            \item Number of links in robot arm (\texttt{n\_links})
        \end{itemize}
        \item Tolerance parameters:
        \begin{itemize}
            \item \texttt{base\_pos\_eps} (this is the $\epsilon$ for how thin/thick the donut of allowed base positions is)
            \item \texttt{base\_heading\_stddev} (large value means the robot can be loosely facing the target, small value means the robot should near exactly face the target)
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Code setup}
The goal is to learn a model for $p(Q_{\text{feat}} \mid C_{\text{feat}})$, where $C_\text{feat}$ encapsulates the featurized form of all information we want to condition on. 
\begin{table}[h]
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Name} & \textbf{Description} & \textbf{Code}: name; dimension \\ \hline
        $Q$ & Complete robot configuration (base position, arm configuration) $\in \mathbb{R}^5$ & \texttt{q\_world}; \texttt{d\_q} \\ \hline
        $Q_{\text{feat}}$ & Featurized robot configuration [input to models] $\in \mathbb{R}^8$ & \texttt{q\_feat}; \texttt{d\_q\_feat}\\ \hline
        $H$ & Target robot hand poses $H$ corresponding to $Q$ [input to models] $\in \mathbb{R}^8$ & \texttt{h\_world}; \texttt{d\_h}\\ \hline
        $C$ & Raw values for conditioning variables (robot end-effector target pose $H$) $\in \mathbb{R}^2$ & \texttt{c\_world}; \texttt{d\_c}\\ \hline
        $C_{\text{feat}}$ & Featurized conditioning information [input to models] $= [0, 0] \in \mathbb{R}^2$ & \texttt{c\_feat}; \texttt{d\_c\_feat}\\ \hline
        $\hat{Q}$ & Given $H$, sampling some matching $\hat{Q}$ from the learned model $p(Q_{\text{feat}} \mid C_{\text{feat}})$ & \texttt{q\_sample} or \texttt{q\_hat}\\ \hline
    \end{tabular}
    \caption{Unified notation}
\end{table}

\subsection{Featurization}
    \[Q = (x, y, \psi, \theta_1, \theta_2)  \mapsto Q_{\text{feat}} = \left(\frac{x - h_x}{R}, \frac{y - h_y}{R}, \cos(\psi), \sin(\psi), \cos(\theta_1), \sin(\theta_1), \cos(\theta_2), \sin(\theta_2)\right)\]

We now prove that this feature vector $Q_{\text{feat}}$ is invariant to translation (moving the robot) and workspace boundary changes (changing the room size).

\begin{proof}[Proof: Translation invariance] Say we train the robot in a lab at base coordinates $(x,y)$ and target coordinates $(h_x, h_y)$. We deploy it in a warehouse at base coordinates $(a,b)$ and target coordinates $(h_a, h_b)$. We want the model to learn a translation invariant mapping, so to the model, $(x,y) \in [-10, 10]^2$ (assuming we bound training data within some arbitrary square) should be equivalent to $(a,b) \in [-10000, 10000]^2$ etc. for all the coordinates. We show below that our $Q_{\text{feat}}$ feature transformation makes it s.t. coordinate scale does not matter to the model (hence achieving translation invariance).
\vspace{1mm}
\par Let the shift vector be $d = (d_x, d_y)$: $(a, b) = (x + d_x, y + d_y), \, (h_a, h_b) = (h_x + d_x, h_y + d_y)$. By our definition of $Q_{\text{feat}}$,
\begin{align*}Q_{\text{feat}} &= \left(\frac{x + d_x - (h_x + d_x)}{R} , \frac{y + d_y - (h_y + d_y)}{R}, \text{angle features}\right) \\
&= \left(\frac{x - h_x}{R}, \frac{y - h_y}{R}, \text{angle features}\right) \Rightarrow \text{in distribution with training data for model!}\end{align*}
\end{proof}

\begin{proof}[Proof: Workspace boundary invariance]
We can also see that our featurization makes the model learned over $p(Q_\text{feat})$ invariant to workspace boundary definition because no features in $Q_{\text{feat}}$ are dependent on attributes of the training workspace:
\[Q_{\text{feat}} = \left(\frac{x - h_x}{R}, \frac{y - h_y}{R}, \cos(\psi), \sin(\psi), \cos(\theta_1), \sin(\theta_1), \cos(\theta_2), \sin(\theta_2)\right) = \left(\frac{\Delta_x}{R}, \frac{\Delta_y}{R}, \ldots\right)\]
By definition, $\Delta$ is invariant to workspace boundary (since all that matters is the relative vector between the base and the target pose).
\end{proof}

\textcolor{violet}{(Sidenote: the previous design of $Q_{\text{feat}}$ in section \ref{v2.0 setup} was translation invariant but \textit{not invariant to workspace boundary}.)}


\subsection{Notes on this setup}

\subsubsection{Collapsing the problem from a conditional generative problem $p(Q \mid H)$ to an unconditional problem $p(Q_{\text{feat}})$} \label{collapse to unconditional}

Our definition of $Q_{\text{feat}}$ essentially performs a change of variables (coordinate canonicalization):
\begin{itemize}
    \item \textbf{The conditional view (allocentric/world frame):} We model $p(Q \mid H_{\text{global}})$. The density depends on $H$ because the ``cloud'' of valid solutions moves around the room as $H$ moves.
    \item \textbf{The relative view (egocentric/canonical frame):} We define $\Delta = Q[0:2] - H_{\text{global}}$ (normalized). We model $p([\Delta, \text{angle features}] \mid H_{\text{global}})$. 
\end{itemize}

If the workspace is infinite and empty, the distribution of valid relative poses does not change depending on where you are in the room, i.e. $p([\Delta, \text{angle features}] \mid H_{\text{global}}) = p([\Delta, \text{angle features}])$. Our current workspace is infinite and empty, so we don't actually need to condition on $H_{\text{global}}$ anymore $\rightarrow$ conditional model has become unconditional!

\subsubsection{Why we keep the conditional architecture}
Given the analysis in section \ref{collapse to unconditional}, why do we keep the conditional architecture in the code?

\begin{center}
    While the problem is unconditional in an infinite void, it will become conditional when we move into the ``real world.'' 
\end{center}
E.g.:
\begin{itemize}
    \item \textbf{Target orientation:} Currently, the target $H$ is a point $(x,y)$. In the real world, targets have orientation (e.g., the handle of a mug).
    \begin{itemize}
        \item Relative position $\Delta$ is not independent of the mug's rotation $\rightarrow$ will need to condition\\ $p([\Delta, \text{angle features}] \mid H_{\text{angle}})$
    \end{itemize}
    \item \textbf{Obstacles:} In a real environment, the validity of a stance $\Delta$ depends on nearby obstacles.
    \begin{itemize}
        \item If there is a wall 0.5m to the right of the target, you can't stand there $\rightarrow$ will need to condition on a ``local occupancy grid'' etc. $p([\Delta, \text{angle features}] \mid \text{LocalMap})$
    \end{itemize}
\end{itemize}
etc.! So we keep our model with a conditional architecture despite not feeding any conditions in for the current infinite void setup. In the future, we can potentially condition on \[H_{\text{context}} = [\cos(\theta_{\text{target}}), \sin(\theta_{\text{target}}), \text{dist\_to\_nearest\_obstacle}, \ldots]\]

\subsubsection{Egocentric learning (canonicalization) papers}
Read more on this! 
\begin{itemize}
    \item SE(3)-equivariant networks (manually enforce SE(2) invariance)
    \item \href{https://research.google/blog/rearranging-the-visual-world/}{Transporter networks} (solve robotic manipulation by learning a ``kernel'' of the object in a canonical frame and then sliding it across the image (convolution) to find a match)
    \item Residual policy learning (often learns a delta-action in a local frame rather than absolute actions)
\end{itemize}

\subsection{Training} Refer to section \ref{v2.0 training}.


\section{Simplified problem v2.0: Round thing with a rotary 2-link arm} \label{v2.0}

\subsection{Setup} \label{v2.0 setup}

\begin{itemize}
    \item \colorbox{lime!30}{Data generation pipeline:}
    \begin{enumerate}
        \item Sample the target $H = (h_x, h_y)$ from our desired task distribution: $$H \sim \text{Uniform}(\text{workspace bounds})$$
        \item Sample base position $(x,y)$ that can reach $H$ via sampling from the ``donut'' of valid positions:
        \begin{itemize}
            \item For the arm to reach $H$, the base must be at a distance $r$ away from $H$ where $r \in [R_{\text{min}}, R_{\text{max}}]$ and $R_{\text{min}} = |L_1 - L_2|$ (folded in), $R_{\text{max}} = L_1 + L_2$ (fully extended). 
            \item We also want to avoid $R_{\text{min}}$ and $R_{\text{max}}$ exactly in the real world to avoid singularities, so we set some arbitrary $\epsilon$ as a proxy for manipulability (require the base sit some distance $r \in [R_{\text{min}} + \epsilon, R_{\text{max}} - \epsilon]$ away from $H$)
            \item $\uparrow$ This sampling is implemented as:
            \begin{align*}
                r &\sim \text{Uniform}(R_{\text{min}} + \epsilon, R_{\text{max}} - \epsilon) \\
                \phi &\sim \text{Uniform}(0, 2\pi) 
            \end{align*}
            Computing base $(x,y)$ coordinates from polar coordinates:
            \[\boxed{x = h_x + r \cos (\phi), \quad y = h_y + r \sin (\phi)}\]
        \end{itemize}

        \item We assume the robot should be somewhat facing the target:
        \begin{itemize}
            \item Perfectly facing the target: $\psi_{\text{ideal}} = \text{atan2}(h_y - y, h_x - x)$\
            \item Add noise to prevent the model from collapsing to a single deterministic mapping: 
            \[\boxed{\psi = \psi_{\text{ideal}} + \delta}, \quad \delta \sim \mathcal{N}(0, \sigma^2)\]
            where heading variance $\sigma^2$ is defined at model initiation (can manually pick).
        \end{itemize}

        \item Analytical arm IK (solve for $\theta_1, \theta_2$): Now that base $B = (x, y, \psi)$ is fixed, transform $H$ into the robot's local frame:
        \begin{enumerate}
            \item Local target: $h_\text{local} = R(\psi)^\top (H - B[0: 2]) \coloneq (u,v)$
            \item Solve $\theta_2$ (elbow): Using law of cosines: \[\cos(\theta_2) = \frac{u^2 + v^2 - L_1^2 - L_2^2}{2L_1 L_2} \coloneq m\]
            There are two solutions $\uparrow \quad \boxed{\theta_2^+ = \arccos(m), \theta_2^- = -\arccos(m)}$ since $\cos(x) = \cos(-x)$
            \item Solve $\theta_1$ (shoulder): \[\boxed{\theta_1 = \text{atan2}(u,v) - \text{atan2}(L_2 \sin \theta_2, L_1 + L_2 \cos\theta_2)}\]
        \end{enumerate}

        \item At this step, we have generated $Q = (x, y, \psi, \theta_1, \pm\theta_2)$ for $H$! To enforce joint limits, we can filter to only keep $(Q, H)$ where $\theta_1, \theta_2 \in [\text{joint limits}]$ in the dataset.
        \item \textit{Future additions:} If we want to address real world priors by biasing the dataset, i.e. adding cost functions for manipulability, clearance, etc.: we can incorporate these into dataset generation! For example, if we have some manipulability scoring function $S(Q)$, we can hard filter out $Q$ where $S(Q) < \text{threshold}$ or soft filter accept $Q$ with probability $P = \frac{S(Q)}{S_{\text{max}}}$
    \end{enumerate}
    \item \colorbox{pink!30}{Modeling assumptions:}
    \begin{itemize}
        \item Robot heading should face the target $H$
    \end{itemize}
    \item \colorbox{cyan!20}{Featurization} (elaborated in section \ref{v2.0 featurization}, \textcolor{red}{the way that we implement coordinate featurization results in model dependence on workspace size, as explained below} \textcolor{blue}{$\rightarrow$ next step is to deprecate this normalization method!!})
    \[Q = (x, y, \psi, \theta_1, \theta_2)  \mapsto \boxed{Q_{\text{feat}} = (g(x), g(y), \cos(\psi), \sin(\psi), \cos(\theta_1), \sin(\theta_1), \cos(\theta_2), \sin(\theta_2))}\]
    \[H = (h_x, h_y) \mapsto \boxed{H_{\text{norm}} = (g(h_x), g(h_y))}\]
    where $g$ is a coordinate-normalization function to bound $x,y$ coordinates within a normalized range. 

    \item \colorbox{violet!15}{Model learning setup:}
    \begin{itemize}
        \item Input: $Q_{\text{feat}}, H_{\text{norm}}$
        \item Goal: Model learns distribution $p(Q_\text{feat} \mid H_{\text{norm}})$ that maximizes the predicted probability of seeing the ground truth data distribution $(Q_{\text{feat}}, H_\text{norm})$ pairs.
    \end{itemize}
\end{itemize}

\subsubsection{Critical error with this setup}
\textcolor{red}{\textbf{Problem:} This setup currently is dependent on the workspace being a nice rectangle for us to do normalization on the $(x,y)$ coordinates.} We have two methods $g(x,y)$ for normalizing coordinates right now that are mathematically equivalent except scaled, and both of them are problematic for learning a model that generalizes across varying workspace bounds:
\begin{itemize}
    \item Bound normalization (\texttt{bound}): 
    \[x \mapsto 2 \cdot \frac{x - x_{\min}}{x_{\max} - x_{\min}} - 1, \quad y \mapsto 2 \cdot \frac{y- y_{\min}}{y_{\max} - y_{\min}} - 1\]

    \item Standardization with geometric $z$-score (\texttt{standardize}): 
    \[x_{\text{center}} = \frac{x_{\max} + x_{\min}}{2}, \quad y_{\text{center}} = \frac{y_{\max} + y_{\min}}{2}, \quad x_{\text{scale}} = \frac{x_{\max} - x_{\min}}{4}, \quad y_{\text{scale}} = \frac{y_{\max} - y_{\min}}{4}\]
    \[x \mapsto \frac{x - x_{\text{center}}}{x_{\text{scale}}}, \quad y \mapsto \frac{y - y_{\text{center}}}{y_{\text{scale}}}\]
\end{itemize}
We can show that these two are the same normalization method up to a constant scale factor: define $W = x_{\text{max}} - x_{\text{min}}$, $C = (x_{\text{max}} + x_{\text{min}})/2$.
\begin{align*}\text{Bound: } x \mapsto 2 \cdot \frac{x - x_{\min}}{x_{\max} - x_{\min}} - 1 &= \frac{2(x-x_{\text{min}})}{w} - \frac{w}{w}  \\
    &= \frac{2(x-x_{\text{min}}) + x_{\min} - x_{\max}}{W} = \frac{2x - x_{\min} - x_{\max}}{W} = \boxed{\frac{2(x-C)}{W}} \\
\text{Standardize: } x \mapsto \frac{x - x_{\text{center}}}{x_{\text{scale}}} &= \frac{x - \frac{x_{\max} + x_{\min}}{2}}{\frac{x_{\max} - x_{\min}}{4}} = \frac{4x - (2x_{\max} + 2x_{\min})}{x_{\max} - x_{\min}} = \boxed{\frac{4(x - C)}{W}}\end{align*}
    
The problem with this normalization method is that we convert absolute meters into relative ``room units'': the model sees the robot's reach not as ``1.5 meters,'' but as ``0.15 room units.''
\begin{itemize}
    \item If we change the workspace size from $10 \times 10$m to $20 \times 20$m: the model still thinks the arm reaches ``0.15 room units,'' 0.15 room units went from 1.5m in $10 \times 10$ room to 3m $20 \times 20$ room, yet the physical robot arm is still only 1.5 meters long. \textcolor{red}{The result is that the model will place the base too far away from the target when we move to a bigger room.}
\end{itemize}

Let us walk through what the model is learning in this situation:
\begin{itemize}
    \item \textbf{The physical truth:} The robot has a fixed physical reach $L_{\text{arm}}$ (e.g., 1.5m). For target $h$ to be reachable from base position $b$, we must have:
    \[||b - h|| \leq L_{\text{arm}}\]

    \item \textbf{The model's truth:} The model receives normalized coordinates $\hat{b}$ and $\hat{h}$. It learns a relationship between them based on the training data: \textit{the model does not know meters, it only knows ``normalized units.''} The model effectively learns a normalized reach threshold $K_{\text{learned}}$: \textcolor{cyan}{(to think about more: how would a cVAE/cINN represent this architecturally? what would the gaussians need to look like?)}
    \[||\hat{b} - \hat{h}|| \leq K_{\text{learned}}\]

    \item \textbf{The problem:} Now, let's substitute the normalization formula into the network's learned truth. Since $\hat{x} = \frac{4(x-C)}{W}$ (the scaling by 4 or 2 doesn't matter for the math below):
    \[||\hat{b} - \hat{h}|| = \bigg|\bigg|\frac{4(b - C)}{W} - \frac{4(h - C)}{W}\bigg|\bigg| = ||b - h||\cdot \frac{4}{W} \]
    Rearranging to see what physical distance the model \textit{thinks} corresponds to reach:
    \[||b - h|| \leq K_{\text{learned}} \cdot \frac{W}{4}\]
    \textcolor{red}{$\uparrow$ This shows a critical issue:} The robot's physical reach is now defined as a percentage of the room width. A concrete example of this going wrong:

    \begin{itemize}
        \item \textbf{Training:} You train in a 10m room ($W = 10$). The robot arm is 1.5m. The model learns $K_{\text{learned}} = 0.6$ (because $0.6 \times \tfrac{10}{4} = 1.5$)
        \item \textbf{Testing:} You deploy in a 20m room ($W = 20$). The model still applies the rule $K_{\text{learned}} = 0.6$.
        \begin{itemize}
            \item The model generates a base position such that the normalized distance $||\hat{b} - \hat{h}||$ is 0.6.
            \item Converting back to physical units: distance = $0.6 \times \tfrac{20}{4} = 3$ meters.
            \item Result: The model places the robot 3 meters away from the target, but the robot arm is still only 1.5m long $\Rightarrow$ \textcolor{red}{robot cannot reach the target!}
        \end{itemize}
    \end{itemize}
\end{itemize}

\textbf{TL;DR:} This model will not generalize to workspaces that are of different size than the one the training data came from!

\subsubsection{Fixing this setup} See section \ref{v2.1 setup}.

\subsection{Forward kinematics}

\subsubsection{\texttt{fk\_mse\_from\_qfeat} loss function math}

Input to function: \texttt{mu\_q} ($\mu_q$ = model's prediction of the robot's state) and $H$. We want to take this prediction, convert it into a valid physical configuration, calculate where the arms' tip should be in the world and compare that to a target position $H$. \textcolor{red}{Write out nicely later.}

\subsection{Training} \label{v2.0 training}

\subsubsection{Nearest neighbors baseline} 

\texttt{
=== Results: NNDeterministicLookup ===\\
hand\_err/mean                  0.014401\\
hand\_err/median                0.013445\\
hand\_err/p95                   0.028278\\
coverage/max\_gap\_mean          6.283185\\
coverage/max\_gap\_p95           6.283185\\
coverage/kl\_to\_uniform         0.019053\\
stochasticity/var\_Q\_fixed\_H    0.000000}

\subsubsection{Conditional VAE}

\textbf{Metrics to track:}
\begin{itemize}
    \item Total loss
    \item Reconstruction loss term
    \item KL loss term
    \item Mean of $|\mu|$: if near 0 always, encoder might be collapsing
    \item Mean of logvar: if very negative $\rightarrow$ tiny standard deviation $\rightarrow$ near-deterministic encoder
    \item Gradient norm: detect instability or dead training
    \item KL per dimension to show if any of the latent dimensions are ``dead'' (near-zero KL contribution)
    
    \textit{Why track $D_{KL}$(Standard normal $||$ latent dimension i) across all latent dimensions?}
    \begin{enumerate}
        \item This KL term measures the ``cost'' of sending information through the latent channel:
        \begin{itemize}
            \item High KL: The encoder is writing a lot of information into this dimension (it deviates strongly from the prior)
            \item Zero KL: The encoder is outputting prior $\mathcal{N}(0, 1)$ regardless of the input. This dimension is ``dead.''
        \end{itemize}
        It is common for VAEs (especially with powerful decoders) to suffer from \textit{posterior collapse} where the decoder decides it is powerful enough to guess the output without looking at the latent code $z$. When this happens, the encoder ``gives up'' and just outputs noise (the prior) to satisfy the KL loss perfectly [since the collective loss is reconstruction + KL --- if reconstruction loss is the same regardless of what we have for $z$, the model will learn to output $z$ that induces minimal KL loss, i.e. directly output the prior over $z$]. Thus, if we see all KL dimensions = 0, the model has collapsed: the encoder is learning nothing. 
        \item The manifold hypothesis (sparsity):
        \begin{itemize}
            \item Real-world data usually lies on a lower-dimensional ``manifold.'' 
            \item E.g., you are modeling a 2D robotic arm with 2 DoF. You set the model's latent dim to 10. Ideally, you want the model to discover that only 2 dimensions are needed. 
            \begin{itemize}
                \item Good result: 2 dimensions have high KL (encoding the angles), and 8 dimensions have KL $\approx 0$ (noise).
                \item Bad result: The information is smeared across all 10 dimensions (entanglement)
            \end{itemize}
        \end{itemize}
        We generally prefer disentangled representations where specific latent dimensions map to specific physical properties. If \texttt{kl\_dim3} spikes only when the robot rotates its wrist etc., that dimensions has learned a specific semantic feature.
        \item Auto-pruning: Because the VAE objective function is Loss = Reconstruction + $\beta \cdot \text{KL}$, the $\beta \cdot \text{KL}$ term acts as a penalty for using latent capacity. It forces the model to be efficient. It effectively asks the model: ``Is this extra dimension necessary to reduce reconstruction error? If not, set it to the prior (KL = 0) to save cost.''
    \end{enumerate}
\end{itemize}

\textit{How is \texttt{z\_dim} usually determined?}
\begin{itemize}
    \item Partly an art and partly science! Conceptually, \texttt{z\_dim} represents the number of independent variables the model needs to describe the data. Some standard methods:
    \begin{enumerate}
        \item Physical approach (intrinsic dimensionality): e.g. count the number of DoF of the system! Set \texttt{z\_dim} slightly higher than the intrinsic dimension (number DoF) to allow the model some ``slack'' to model noise/non-linearities that aren't perfectly captured by the physics.
        \item The ``active units'' approach (pruning):
        \begin{enumerate}
            \item Overshoot: Set \texttt{z\_dim} larger than necessary.
            \item Regularize: Use $\beta$-VAE (where $\beta >1$)
            \item Inspect: Look at the \texttt{kl\_per\_dim} metrics.
        \end{enumerate}
        Because the KL term penalizes information storage, the VAE will naturally try to ``shut off'' unnecessary dimensions (drive their KL to 0). E.g., if we set \texttt{z\_dim = 16} and see that 5 dimensions have high KL and 1 have near-zero KL, the model has told us that the true dimensionality is 5. We can then retrain with \texttt{z\_dim = 5} (or 6 etc.) for a more compact model.

        \item Hyperparameter sweep (the elbow plot): If we don't know the physics of the problem (e.g., generating images of faces), we can run a grid search. Train models with \texttt{z\_dim = [2, 4, 8, 16, 32, 64]} and plot the reconstruction loss (NLL) vs \texttt{z\_dim}. 
        \begin{itemize}
            \item Low \texttt{z}: High error (underfitting). The bottleneck is too tight
            \item High \texttt{z}: Low error, but diminishing returns with increasing \texttt{z} at some point.
            \item The ``elbow'': The point where adding more dimensions stops significantly lowering the reconstruction error = optimal \texttt{z\_dim}!
        \end{itemize}
    \end{enumerate}
\end{itemize}

\textit{What prevents a VAE from memorizing instead of learning when \texttt{z\_dim} = exact number DoF in system?}
\begin{itemize}
    \item If you gave a standard Autoencoder \texttt{z\_dim = 5}, it would simply memorize the dataset $\rightarrow$ it would essentially build a hash map where \texttt{Input A} maps to \texttt{Point A} in latent space. 
    \item The VAE prevents this through (1) sampling noise and (2) the KL penalty.
    \begin{enumerate}
        \item Sampling noise: A standard autoencoder maps an input datapoint to a specific point in latent space (e.g., ``This specific robot pose is exactly coordinate \texttt{(2.1, -0.5)}''). Contrarily, a VAE maps an input datapoint to a probability distribution in latent space (``This specific robot pose is somewhere in the region centered at \texttt{(2.1, -0.5)} with a radius of 0.1''). This prevent memorization: because of the sampling step ($z = \mu + \sigma \cdot \epsilon$), the decoder never sees the same input twice $\rightarrow$ in epoch 1, the latent $z$ corresponding to $x$ might be 2.1, then 2.05 in epoch 2, 2.11 in epoch 3, etc. $\rightarrow$ the decoder cannot just memorize ``When I see \texttt{2.1}, output $x$,'' forcing generalization rather than memorization.
        \item The KL penalty: 
        \begin{itemize}
            \item \textit{Q: What prevents the encoder from making the cloud around $\mu$ really, really small?} If the encoder shrinks the variance to zero, the cloud becomes a single point, and the VAE turns back into a standard AE that can memorize everything.
            \item $\uparrow$ \textit{But the KL divergence term prevents this!} KL loss for a Gaussian $\coloneq D_{KL} = 0.5 \sum (1 + \log (\sigma^2) - \mu^2 - \sigma^2) \Longrightarrow$ if the model tries to memorize by making $\sigma \rightarrow 0$, $\log(0) \rightarrow -\infty$ and the loss goes to infinity.
        \end{itemize}
    \end{enumerate}
    So for VAEs there is a tradeoff during training! To get low reconstruction error (memorization), the model wants to shrink $\sigma$ to be precise. To get low KL error (regularization), the model wants to expand $\sigma$ to be 1 (unit standard normal). TL;DR: Sampling + KL penalty collectively prevent the VAE to act like a lookup table even when \texttt{z\_dim} $\geq$ true DoF in the system.
\end{itemize}


\subsubsection{Conditional INN} cINNs are trained using MLE: we want to maximize the probability of the data point $x$ given condition $c$. By using the change of variables formula, the log-likelihood is:
$$\log p_X (x \mid c) = \log p_Z(z) + \log |\det J|$$
where:
\begin{itemize}
    \item $z = f(x; c)$ is the latent vector mapped to by the model
    \item $p_Z(z)$ is the base distribution (usually a standard Normal $\mathcal{N}(0, I)$)
    \item $\log |\det J|$ is the log-determinant of the Jacobian (measuring how much the network expands/contracts volume)
\end{itemize}

Since we minimize NLL, the loss function is:
\[\mathcal{L} = \underbrace{- \log p_Z(z)}_{\text{z\_term}} + \underbrace{(- \log |\det J|)}_{\text{logdet\_term}}\]

\textbf{Metrics to track:}
\begin{itemize}
    \item Loss components (Track: \texttt{z\_term}, \texttt{logdet\_term}):
    \begin{itemize}
        \item \texttt{z\_term} is minimal when $p_Z(z) = 1$, i.e. $z = \mu(p_Z)$ (fit the mean of the prior Gaussian)
        \item \texttt{logdet\_term} prevents the model from collapsing all data to a single point: it force the model to maintain volume \textcolor{cyan}{(read into the math behind this more)}
        \begin{itemize}
            \item Watch for: if \texttt{logdet\_term} becomes extremely negative (e.g. $-10^6$) while \texttt{z\_term} becomes massive, the model is expanding volume infinitely to ``cheat'' the loss.
        \end{itemize}
    \end{itemize}
    \item Latent statistics (Track: \texttt{z\_sq\_mean}, \texttt{z\_norm\_mean})
    \begin{itemize}
        \item We are forcing $z$ to look like $\mathcal{N}(0, 1)$, so in theory, we know exactly what the statistics of $z$ should be: 
        \begin{itemize}
            \item \texttt{z\_sq\_mean}: since the variance of a standard normal is 1, this value should converge to roughly 1.0
            \item \texttt{z\_norm\_mean}: if this explodes (e.g., $>100$), the model is mapping some inputs to very extreme regions (since supposedly $z \sim \mathcal{N}(0, 1)$), indicating instability or outliers in the training data.
        \end{itemize}
    \end{itemize}
    \item Jacobian statistics (Track: \texttt{log\_detj\_std}, \texttt{log\_detj\_min/max})
    \begin{itemize}
        \item This measures the ``stiffness'' of the transformation \textcolor{cyan}{(read into the theory behind this more)}
        \begin{itemize}
            \item \texttt{log\_detj\_std}: if this is very high, the network is reacting very differently to different samples in the batch $\rightarrow$ this suggests the training surface is rugged or the batch size is too small. 
            \item \texttt{log\_detj\_min/max}: spikes here often precede \texttt{NaN} errors
        \end{itemize}
    \end{itemize}

    \item Gradient norms (Track: \texttt{grad\_norm\_prev\_clip}, \texttt{grad\_norm\_post\_clip})
    \begin{itemize}
        \item Measures how big the update steps are!
        \item Health check: \texttt{grad\_norm\_prev\_clip} should eventually settle down $\rightarrow$ if it consistently hits your clip value (causing \texttt{post\_clip} to be capped), the learning rate is likely too high
    \end{itemize}
    \item Validation NLL:
    \begin{itemize}
        \item In theory, cINNs can memorize the training data and achieve arbitrarily low NLL by shrinking the volume around training points. 
        \item Track this using a held-out validation set during training: if train NLL keeps dropping but val NLL plateaus/rises, stop training!
    \end{itemize}

    \item \textcolor{blue}{To add:}
    \begin{itemize}
        \item Reconstruction error (invertibility check): in theory, cINNs are strictly invertible. Ensure that $x \approx f^{-1}(f(x))$: if this error is high, the ``generative'' capabilities of the INN are broken, even if NLL is low. 
        \begin{lstlisting}
with torch.no_grad():
    z = model(x, c)
    x_recon, _ = model.reverse(z, c) # Assuming reverse method exists
    recon_error = torch.mean((x - x_recon)**2)
    # Log "train/recon_mse": recon_error.item()
        \end{lstlisting}
        \item Latent mean: We track \texttt{z} variance, but even if the model learns a variance of 1.0, if it centers the cloud at $z = 50$, this is still a failure mode $\rightarrow$ the mean of \texttt{z} should be 0: track \texttt{"train/z\_mean": z.mean().item()} and ensure that it hovers around 0
    \end{itemize}
\end{itemize}

\subsubsection{Conditional diffusion} Diffusion models are trained using MSE loss: $|| \epsilon - \epsilon_\theta(x_t, t)||^2$. Because the loss function forces the model prediction $\epsilon_\theta(x_t, t)$ to match $\epsilon$, the model's output should also roughly approximate $\mathcal{N}(0, I)$. 

\textbf{Metrics to track:}
\begin{itemize}
    \item Predicted noise distribution: Since target noise $\epsilon$ is drawn from $\mathcal{N}(0, 1)$, the empirical mean and standard deviation of the model's prdictions $\hat{\epsilon}$ should be: \[\mathbb{E}[\hat{\epsilon}_\theta] \approx 0, \quad \text{Std dev.}[\hat{\epsilon}_\theta] \approx 1\]
    \begin{itemize}
        \item Mean $\neq$ 0: The model is systematically biased (predicting drift)
        \item Std $<< 1$ (denoising too timid): The model is outputting values too close to zero. This is a common failure mode in diffusion called ``regression to the mean,'' where the model gives up and predicts the average noise (0) instead of the actual noise structure. 
        \item Std $>> 1$ (denoising too aggressive): Gradient explosion or scaling instability.
    \end{itemize}
    \item Loss by timestep: The diffusion loss varies depending on the timestep $t$ (bucket into time chunks)
    \begin{itemize}
        \item High $t$ (near max steps): The image is mostly noise. Loss is often high because $x_t$ contains very little signal about $x_0$.
        \item Low $t$ (near 0): The image is mostly signal. Loss should be very low.
    \end{itemize}

    \item Signal-to-noise ratio weighting: \[\text{SNR}(t) = \frac{\bar{\alpha}_t}{1 - \bar{\alpha}_t}\]
    where $\bar{\alpha}_t$ is the cumulative product of $1 - \beta_t$ from the scheduler. Logging the SNR helps verify that the noise scheduler is actually destroying the signal correctly.
\end{itemize}


\subsection{Additional notes}

\textcolor{red}{\textbf{Featurization section below is now deprecated}, featurizing this way causes the model to depend on workspace size.}

\subsubsection{Featurization} \label{v2.0 featurization}
We featurize data $Q = (x, y, \psi, \theta_1, \theta_2) \in \mathbb{R}^5$ as:
\[Q_{\text{feat}} = (x_\text{norm}, y_\text{norm}, \cos(\psi), \sin(\psi), \cos(\theta_1), \sin(\theta_1), \cos(\theta_2), \sin(\theta_2)) \in \mathbb{R}^8\]

We also featurize $H = (h_x, h_y) \mapsto H_{\text{feat}} = (h_{x, \text{norm}}, h_{y, \text{norm}})$ to ensure that the features being conditioned upon are in the same scale.

\begin{enumerate}
    \item Angle featurization of angle $\mapsto \cos(\text{angle}), \sin(\text{angle})$ is to bound the values within $[-1, 1]$ and avoid the wrapping problem (where the model outputting 0 is equivalent to $2\pi$ but numerically at the two ends).
    \item Base position $(x,y)$ is normalized to avoid arbitrary weighing between rotation features vs. position features (i.e., 2000mm will look a lot ``bigger'' to the model than 2m, especially relative to the bounded angle features). Two normalization methods are implemented:
    \begin{itemize}
        \item Bounding (\texttt{bound} in the code): This method takes any rectangular workspace and squashes it down to a $[-1, 1]^2$ box \[x \mapsto 2 \cdot \frac{x - x_{\min}}{x_{\max} - x_{\min}} - 1, \quad y \mapsto 2 \cdot \frac{y- y_{\min}}{y_{\max} - y_{\min}} - 1\]
        \item Standardization with geometric $z$-score (\texttt{standardize} in the code): Define
        \[x_{\text{center}} = \frac{x_{\max} + x_{\min}}{2}, \quad y_{\text{center}} = \frac{y_{\max} + y_{\min}}{2}, \quad x_{\text{scale}} = \frac{x_{\max} - x_{\min}}{4}, \quad y_{\text{scale}} = \frac{y_{\max} - y_{\min}}{4}\]
         \[x \mapsto \frac{x - x_{\text{center}}}{x_{\text{scale}}}, \quad y \mapsto \frac{y - y_{\text{center}}}{y_{\text{scale}}}\]
         Notice the effect of this normalization is that:
         \[x_{\min} \mapsto \frac{(x_{\min} - x_{\max})/2}{(x_{\max} - x_{\min})/4} = -2, \quad x_{\max} \mapsto \frac{(x_{\max}- x_{\min})/2}{(x_{\max} - x_{\min})/4} = 2\]
         And the same for $y$! So the workspace has effectively been standardized down to $[-2, 2]$.
    \end{itemize}
\end{enumerate}

Empirically, we observe that method 2 (standardization) leads to much better cINN training performance (\texttt{hand\_err\_mean} of 0.28 vs. 0.062 when all other training conditions are held constant), whilst both lead to similar cVAE performance. Some theories as to why the cINN approach greatly prefers method 2 for base position normalization.

\section{Simplified problem v1: Round thing with a fixed stick}

\subsection{Setup}

Simple robot is a round thing with a fixed ``stick'' attached to it: a disk on the floor with a rigid stick of fixed length $L$ glued to it, and the stick rotates with the disk.
\begin{itemize}
    \item Configuration: $Q = x, y, \theta$ ($\mathbb{R}^2 \times S^1$)
    \item Hand target: $H = h_x, h_y$ (point in $\mathbb{R}^2$)
\end{itemize}

In this setup, $H \in \mathbb{R}^2$ is the desired 2D point on the floor where we want the tip of the stick to be.

\subsection{Forward kinematics}
\[\text{hand}(x, y, \theta) \coloneq f(Q) = \begin{bmatrix}
    x \\ y
\end{bmatrix} + L \begin{bmatrix}
    \cos \theta & \sin \theta
\end{bmatrix}\]

So the IK constraint (``hand hits the target'') is:
\[H = \begin{bmatrix}
    h_x \\ h_y
\end{bmatrix} = \begin{bmatrix}
    x \\ y 
\end{bmatrix} + L \begin{bmatrix}
    \cos \theta \\ \sin \theta
\end{bmatrix}\]

Rearranging shows that as $\theta$ varies, the base center $(x,y)$ traces out a circle of radius $L$ around the target point $H$: the set of feasible base positions for the robot to reach $H$ is a circle centered at $H$. 
\[\text{Ground truth feasible set for a fixed $H$} \coloneq \{Q: f(Q)= H\}= \{(h_x - L\cos \theta, h_y - L \sin\theta, \theta): \theta \in [0, 2\pi)\}\]

% Simple robot is a round thing with a fixed stick attached to it
% - configuration : Q = x, y, theta
% - hand target : H = x, y point

\subsection{Approaches} We try the following approaches to model $p(Q \mid H)$:
\begin{enumerate}
    \item \textit{Nearest neighbors} (baseline):
    \begin{itemize}
        \item Given a dataset $\mathcal{D} = \{(H_i, Q_i)\}_{i=1}^N$: for a query $H'$,
        \begin{enumerate}
            \item Find indices of the $k$ nearest $H_i$ to $H'$ (under $||H_i - H'||$)
            \item Return their associated $Q_i$'s (either all or sample one etc.)
        \end{enumerate}
        \item For a new $H'$, we can return the $Q_i$ associated to the nearest $H_i$, or sample from an empirical conditional distribution created based on the nearest neighbor graph:  $\hat{p}_{\text{kNN}}(Q \mid H') = \sum_{j \in \mathcal{N}_k(H')} w_j (H') \delta (Q - Q_j)$ (where weight $w_j$ is proportional to the distance between $H'$ and $H_i$, i.e. $w_j \propto \exp(-||H_j - H'||^2 / \sigma^2)$).
    \end{itemize}
    \item \textit{Conditional VAE:} Introduce latent $z \in \mathbb{R}^d$: $z \sim \mathcal{N}(0, I), \,\, Q \sim p_\theta(Q \mid H, z)$
    \begin{itemize}
        \item Train an encoder $q_\phi(z \mid Q, H)$ and decoder $p_\theta(Q \mid H, z)$ via ELBO loss:
        \[\log p_\theta (Q \mid H) \geq \mathbb{E}_{z \sim q_\phi(z \mid Q, H)} [\log p_\theta (Q \mid H, z)] - \mathrm{KL}[q_\theta(z \mid Q, H)\, || \,\mathcal{N}(0, I)]\]
        \item Potential decoder choice: output $\mu_\theta(H, z)$ and diagonal $\Sigma_\theta(H, z)$; use Gaussian likelihood: \[\log p_\theta (Q \mid H, z) = \log \mathcal{N}(Q; \mu_\theta (H, z), \Sigma_\theta (H, z))\]
    \end{itemize}
    \textcolor{red}{[TODO: Read into math more]}, notes in \ref{cvae}
    \begin{itemize}
        \item Advantages of cVAE for this problem:
        \begin{itemize}
            \item Multi-modality via latent variable $z$ (different $z$'s can map to different valid configurations on the circle) $\rightarrow$ addresses mode collapse relative to deterministic nets \textcolor{blue}{[assuming we avoid posterior collapse (decoder ignores $z$)]}
            \item Smooth generalization in $H$: theoretically, the encoder/decoder can learn continuous maps in $H$-space, allowing better interpolation than the nearest neighbors approach.
        \end{itemize}
        \item Note: $\theta$ is periodic, so training a Gaussian likelihood $p_\theta(Q \mid H, z)$ (in the decoder) on raw $\theta \in [0, 2\pi)$ is awkward because $\theta \approx 0$ and $\theta \approx 2\pi$ are close physically but far numerically ($\hat{\theta} = 2\pi - 0.01$ is a pretty good prediction for ground truth $\theta = 0.01$). To avoid this, instead of modeling $\theta \sim \mathcal{N}(\mu, \sigma^2)$, we can model a Gaussian over Cartesian coordinates in $\mathbb{R}^2$: \[(\cos \theta , \sin \theta) \sim \mathcal{N}(\mu_{cs}, \text{diag}(\sigma^2_{cs}))\]We can recover $\theta$ via $\mathrm{atan2}$ at inference time.
    \end{itemize}
    \item \textit{Invertible NN} (FrEIA): \textcolor{red}{[TODO: Read into math more]}, notes in \ref{inn}
    \begin{itemize}
        \item \textbf{Representation:} To avoid periodicity issues in $\theta$, we model: \[Q_{\text{feat}} = (x, y, \cos \theta) \in \mathbb{R}^4, \quad H = (h_x, h_y) \in \mathbb{R}^2\]
        \item \textbf{Conditional invertible map:} We learn an invertible mapping (for each condition $H$)
        \[f_\theta(\cdot; H): \mathbb{R}^4 \leftrightarrow \mathbb{R}^4, \quad z = f_\theta (Q; H), \quad Q = f_\theta^{-1} (z; H)\]
        with a simple base distribution $z \sim \mathcal{N}(0, I_4)$. In code this is implemented as a stack of conditional affine coupling blocks (FrEIA \texttt{SequenceINN} + \texttt{AllInOneBlock}) where each block receives $H$ as a conditioning input.
        \item \textbf{Training:} Using the change-of-variables formula, the conditional density is: 
        \[p_\theta(Q \mid H) = p_Z (f_\theta(Q; H)) \bigg|\det\frac{\partial f_\theta(Q; H)}{\partial Q}\bigg|\]
        Therefore:
        \[\log p_\theta(Q \mid H) = \log p_Z (z) + \log |\det J_{f_\theta}(x; H) |\]
        With $p_Z = \mathcal{N}(0, I)$, the per-example negative log-likelihood (dropping constants) is:
        \[\mathcal{L}_{\text{NLL}}(Q, H) = \frac{1}{2} ||z||^2 - \log |\det J_{f_\theta}(Q; H)|\]
        We minimize $\mathbb{E}_{(Q, H) \sim \text{data}} |\mathcal{L}_{\text{NLL}} (Q, H)|$.

        \item \textbf{Sampling:} At test time, for a given $H$ we sample:
        \[z \sim \mathcal{N}(0, I_4), \qquad Q = f_\theta^{-1}(z, H)\]
        then convert $Q = (x, y, \cos \theta, \sin \theta)$ back to $(x, y, \theta)$ with $\theta = \text{atan2} (\sin \theta, \cos \theta)$. 

        \item \textbf{Optional FK regularization (sample-space constraint):} Since the true conditional support lies on the IK manifold (all physically valid configurations for a given target must satisfy the IK constraint), we can add a penalty that directly enforces that generated samples reach $H$:
    \[
    \mathcal{L}
    =
    \mathbb{E}_{(Q,H)\sim\mathcal{D}}\big[\mathcal{L}_{\text{NLL}}(Q,H)\big]
    \;+\;
    \lambda_{\text{FK}}\,
    \mathbb{E}_{H\sim\mathcal{D},\, z\sim\mathcal{N}(0,I)}
    \Big[
      \|f_{\text{FK}}(f_\theta^{-1}(z;H)) - H\|^2
    \Big].
    \]
    In our implementation we compute FK error on inverse samples $\hat{Q} = f_\theta^{-1}(z;H)$ during training (rather than on the training $Q$ itself, as $\text{FK}(Q) = H$ by virtue of being in the dataset), so the regularizer shapes the \emph{sampled} conditional distribution.
    \end{itemize}

\end{enumerate}

\subsection{Evaluation} \label{eval simple}
Assuming that the ground-truth $p^*(Q \mid H)$ is a uniform distribution over the feasible circle set, i.e.: $\theta \sim \text{Unif}[0, 2\pi) \rightarrow x = h_x - L\cos\theta, \, y = h_y - L\sin\theta$:
\begin{itemize}
    \item Accuracy: $e_{\text{hand}} (Q, H) \coloneq ||f(Q) - H||$
    \item Diversity/coverage: convert each sample to its implied angle on the circle: \[\theta^{(s)}_{\text{implied}} \coloneq \mathrm{atan2}\left(h_y - y^{(s)}, h_x - x^{(s)}\right)\]
    Check how well the empirical distribution over $\theta$ matches the target (here we assume $\theta \sim \text{Unif}[0, 2\pi)$): for histogram $\hat{p}(\theta)$, we can compute:
    \begin{itemize}
        \item KL divergence to uniform: $D_{\mathrm{KL}} (\hat{p}(\theta) \, ||\, \text{Unif})$
        \item Max angle gap: sort angles $\theta^{(s)}$ around the circle, compute max gap $\Delta_{\mathrm{max}} = \max_i (\theta_{i+1} - \theta_i)$
        
        Large $\Delta_{\text{max}}$ = model is missing big arcs (collapse)
    \end{itemize}
    \item ``Uses latent'' test (for cVAE/cINN): fix $H$, sample many $z$'s, then measure output variance $\mathrm{Var}(Q \mid H)$. If the model ignores $z$, this variance will be small.\\
    \textcolor{blue}{[$\downarrow$ TODO: Implement]}
    \item Generalization in $H$: make a test set where $H$ is (1) in-distribution, (2) near boundary, (3) out-of-distribution (slightly outside training workspace). Compare success and coverage!
    \item Inference speed per sample
\end{itemize}

\subsection{Extending to more general cases} Ideally, would be nice if this toy problem gave us an idea of the advantages/disadvantages of different approaches in regards to the following challenges:
\begin{itemize}
    \item \textit{Mode collapse:} Test: fixed $H$, sample 1k solultions, compute $\Delta_{\mathrm{max}}$, KL-to-uniform, etc.
    \item \textit{Generalization:} Test: distribution of $e_{\text{hand}} = || f(Q) - H||$
    \item \textit{Bias from data collection} (forward vs. inverse sampling): 
\end{itemize}

\subsection{Potential next-step extensions}
\begin{itemize}
    \item Disconnected feasible sets: modify toy with a ``visibility'' constraint such as \[\text{visible}(Q, H) = 1 \iff \theta \in [\alpha_1, \beta_1] \cup [\alpha_2, \beta_2]\]
    Now the true $p^*(Q \mid H)$ is two separated modes. \textbf{Test:} cluster sampled angles into arcs and compute mode recall: fraction of samples that land in each arc. 
    \item Scalability with dimension: add DoF to the toy (e.g., 2-link arm, variable stick length). \textbf{Test:} track how coverage/error degrades with dimension.
\end{itemize}

\section{Extra notes}

\subsection{Mode collapse}

A generative model has mode collapse if, for a fixed condition $H$, the samples $Q \sim \hat{p}(Q \mid H)$ cover only a small subset of the true support of $p^* (Q \mid H)$. 
\begin{itemize}
    \item In this toy problem, the support is the full circle parameterized by $\theta$. Mode collapse looks like the model always outputting $\theta \approx 0$ (i.e. stands in one favorite location) or outputting only a few discrete angles, ignoring the rest the circle, etc.
\end{itemize}

\subsection{Conditional VAE theory} \label{cvae} The goal is to model a multi-modal conditional distribution over robot configurations $Q$ given target $H$: $p^*(Q \mid H)$. In our simple robot setup, the true conditional has a 1D manifold of solutions (a circle in $(x,y)$ paired with corresponding $\theta$), so it's not unimodal in the usual sense (rather than being a single blob in $\mathbb{R}^d$, for fixed $H$, probability mass $p^*(Q \mid H)$ is concentrated along a ring in $(x,y)$ with a corresponding orientation $\theta$ at each point). 

\subsubsection{Motivation} Because $p^*(Q \mid H)$ is not unimodal, if we model $p(Q \mid H)$ as a single Gaussian/predict a single mean, we face mean collapse: the mean of points around a circle is the center of the circle, but the center is invalid. So the ``best'' unimodal prediction is often physically invalid/low-probability under the true distribution.

\subsubsection{Model architecture} We introduce latent $z \in \mathbb{R}^{d_z}$, which ideally will be the variable that ``chooses'' \textit{which} solution the model means among the many possible solutions on the ring for a given $H$. Define a prior $p(z) = \mathcal{N}(0, I)$. There are two parts to the model (decoder and encoder):
\begin{itemize}
    \item \underline{Decoder (generative model):} The decoder is a conditional distribution $p_\theta(Q \mid H, z)$. For the architecture of the decoder, we use the diagonal Gaussian \textcolor{blue}{(potential next step -- try other architectures)}: \[p_\theta(Q \mid H, z) = \mathcal{N}(Q; \mu_\theta(H, z), \text{diag}(\sigma^2_\theta (H, z)))\] where $\mu_\theta(H,z)$ and $\sigma^2_\theta$ are the predicted mean/variance of each coordinate of $Q$. 
    \item \emph{Interlude:} $p_\theta(Q, z \mid H) = p(z \mid H) p(Q \mid z, H) = p(z) p_\theta(Q \mid H, z)$ [assuming $p(z\mid H) = p(z)$]. Bayes' rule gives:
    \[p_\theta(z\mid Q, H) = \frac{p_\theta(Q, z \mid H)}{p_\theta(Q \mid H)} = \frac{p(z) p_\theta (Q \mid H, z)}{p_\theta(Q \mid H)}\]
    Expanding the denominator: $p_\theta(Q \mid H) = \int p(z') p_\theta (Q \mid H, z') \, dz'$ (consider all possible latent explanations $z'$, weight how likely each would produce $Q$, then sum them up). Plugging this in,
    \[p_\theta(z\mid Q, H) =\frac{p(z) p_\theta(Q \mid H, z)}{\int p(z') p_\theta(Q \mid H, z') \, dz'}\]
    The denominator $\int p(z') p_\theta(Q \mid H, z') \, dz'$ is intractable: $p_\theta(Q \mid H, z') = \mathcal{N}(Q; \mu_\theta(H, z'), \Sigma_\theta(H, z'))$, so we effectively have
    \[p(z') p_\theta(Q \mid H, z') \Rightarrow \mathcal{N}(z'; 0, 1) \times \mathcal{N}(Q; \mu_\theta(H, z'), \Sigma_\theta(H, z'))\]
    For fixed $Q$ and $H$, $\mathcal{N}(Q; \mu_\theta(H, z'), \Sigma_\theta(H, z'))$ becomes some complicated nonnegative function of $z'$ $\rightarrow g(z')$. Then, 
    \[p_\theta(Q \mid H) = \int \mathcal{N}(z'; 0, I) g(z')\, dz'\]
    This is very hard to compute analytically: recall the multivariate Gaussian density is given by:
    \[f_{\mu, \Sigma}(Q) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(- \tfrac12 (Q-\mu)^\top \Sigma^{-1} (Q-\mu)\right)\]
    Here, we plug in $\mu = \mu_\theta (H, z'), \Sigma = \Sigma_\theta (H, z')$. These are neural-net outputs, i.e. complicated nonlinear functions of $z'$, which makes the integral difficult to compute. 

    Since the true posterior is intractable, introduce a tractable approximation $q_\phi(z \mid Q, H) : (Q, H) \mapsto p(z)$ which essentially answers ``If this $Q$ came from some latent $z$, what must that $z$ have been?''

    \item \underline{Encoder (inference model):}  The encoder is the conditional distribution $q_\phi(z \mid Q, H)$. 
\end{itemize}

% \subsubsection{ELBO loss}

\subsection{FrEIA INN theory} \label{inn} The goal is to learn $p(Q \mid H)$, where $Q = (x, y, \cos \theta, \sin \theta) \in \mathbb{R}^4$ (equivalent form of $(x, y, \theta)$) and $H = (h_x, h_y) \in \mathbb{R}^2$. A normalizing flow models an invertible map (for each fixed condition $h$): 
\[f_\theta(\cdot; h) : \mathbb{R}^4 \rightarrow \mathbb{R}^4, \quad z = f_\theta (q; h), \quad q = f_\theta^{-1}(z; h)\]
We pick the base density for $z$ to be the multivariate standard normal: \[p_Z(z) = \mathcal{N}(0, I_4)\]
Then, $p_\theta(q \mid h)$ can be defined via ``push $q$ through $f$ to get $z$, and measure how likely that $z$ is, corrected by volume change'' $\rightarrow$ correction is the Jacobian determinant. 

\subsubsection{Hand-wavy intuition} For each condition $h$, the true data $q$ lives in some weird, potentially multi-modal shape (ring of solutions in our simplified problem). A Gaussian cannot represent that shape directly.
\begin{itemize}
    \item \textit{Core idea:} Instead of trying to fit a complicated distribution directly, we learn a warp of space that makes the complicated shape look Gaussian.
    \begin{itemize}
        \item Forward direction: $z = f_\theta(q; h)$ takes real samples $q$ and maps them into ``Gaussian space''
        \item Training enforces: ``after mapping, these $z$'s should look like samples from $\mathcal{N}(0, I)$''
        \item Warping space changes volume $\rightarrow$ probability densities change:
        \begin{itemize}
            \item If $f$ expands a region of $q$-space, then points there become less dense in $q$-space. 
            \item If $f$ compresses a region, points become more dense.
        \end{itemize}
    \end{itemize}
\end{itemize}
After learning an invertible NN model, we can generate realistic $q$ by:
\begin{enumerate}
    \item Sample $z \sim \mathcal{N}(0, I)$
    \item Invert: $q = f_\theta^{-1}(z; h)$
\end{enumerate}

\textit{Fun way of thinking about it:} Imagine data for $q$'s is shaped like a bent pretzel in $\mathbb{R}^D$. A flow learns a continuous, invertible deformation of space that morphs the pretzel into a $D$-dim. Gaussian blob. Forward pass = ``unbend the pretzel into a Gaussian.'' Inverse pass = ``bend a Gaussian blob back into a pretzel.'' The Jacobian term is bookkeeping of how the deformation stretches space.

\subsubsection{Why layers need to be invertible}
\begin{enumerate}
    \item Invertible layers allow flow models to compute an exact probability for any observed training point: if $Q$ is a real solution from the dataset for some $H$, we can compute exactly how likely the model thinks it is (flows are the only model out of GANs, VAEs, and diffusion models that can compute the exact log-likelihood of a new sample)
    \[\Pr(\mathbf{f} (\mathbf{Q})\mid \mathbf{H}, \mathbf{\phi}) = \bigg|\frac{\partial \mathbf{f} (\mathbf{z}, \mathbf{H}, \phi)}{\partial \mathbf{z}}\bigg|^{-1} \cdot \Pr(\mathbf{z}) \text{ where } \mathbf{z} = \mathbf{f}^{-1} (\mathbf{x}, \mathbf{H}, \phi) \text{ and } \phi = \text{learned model params}\]
    \item Invertibility allows sampling to be one inverse pass rather than iterative denoising, sampling chains, etc.: once trained, we can directly (1) sample $z \sim \mathcal{N}(0, I)$ (2) compute $Q = f_{\theta}^{-1} (z; H)$
\end{enumerate}

\subsubsection{More concrete math} In separate PDF (\texttt{flow model FrEIA implementation notes.pdf}).


\subsection{Normalizing flow} In separate PDF (\texttt{normalizing flow notes.pdf}).

\subsection{Diffusion models}


\subsection{Conceptual comparison of different methods}

Given the data is shaped like a pretzel in $\mathbb{R}^D$ \ldots

\subsubsection{Flow} Learn an invertible deformation conditioned on $H$ that straightens the pretzel into a Gaussian $\mathcal{N}(0, I_D)$. Sampling works by running the inverse deformation on $z \sim \mathcal{N}(0, I_D)$ and $H$ to obtain $Q^*$. 

\subsubsection{cVAE} Learn a conditional generator that uses Gaussian knobs to choose how to output the pretzel; training teaches the knobs to stay Gaussian while reconstructing data.

\subsubsection{Diffusion} Instead of unbending the pretzel into a Gaussian in one shot, diffusion dissolves the pretzel into pure noise by gradually adding noise, then learns to sculpt noise back into the pretzel step-by-step.

\subsection{Invertible neural networks vs. normalizing flow}

Invertible neural networks are a class of deep networks that approximate bijective functions and are characterized by a forward mapping that can be inverted. All normalizing flow networks are invertible neural networks. 

\subsubsection{INNs as generative models} Because INNs learn a transformation between the input data distribution and a simple prior distribution (typically a normal distribution), the inverse mapping automatically acts as a generator, converting the simple distribution into samples resembling the input data. 

\subsubsection{INNs that are not normalizing flow models} Invertible neural networks that are not flow models exist \textcolor{purple}{(TODO: Read into this paper \cite{INN2019})}.

% All normalizing flow networks are invertible neural networks! Here is the math behind why:


\pagebreak
\bibliographystyle{plain} % or another style you prefer
\bibliography{reachability_references}       % Note: No need to include the .bib extension

\end{document}
